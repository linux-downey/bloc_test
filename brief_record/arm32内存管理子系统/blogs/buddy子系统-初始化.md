# buddy 子系统 - 初始化

在内核的内存初始化阶段，memblock 在完成了一些最基本的物理内存信息收集以及必要的内存分配之后，就需要着手开始向 buddy 系统进行迁移了，毕竟 buddy 子系统越早启动，内核就越早进入到内存管理的正轨。

但是这个过程并不是一步就位的，内存所涉及到的方方面面本身是非常复杂的，尽管 buddy 系统已经足够简单，那也只是相对其它的内存管理器而言，本章着重于分析 buddy 子系统的初始化的早期部分，也就是一些准备工作。 

## 背景

memblock 作为内核早期的内存管理器，有几个重要的阶段：

* 在内核中被静态定义，整个内存管理器由 memory 和 reserved 两个数组来管理物理内存

* 扫描设备树，获取系统中所有物理内存信息
* 为内核镜像、dtb、设备树中指定内存等区域设置保留空间，并针对一些必要的部分分配物理内存
* 为所有物理内存建立页表，不过建立页表的工作并不完全算 memblock 来做的，它只是提供相应的内存信息，总之，在此之后，系统中所提供的所有物理内存及其属性都已经确定，线性映射区可以访问了。

在这种情况下，buddy 子系统已经具备了启动的环境，memblock 开始慢慢地将内存管理的重任交接给 buddy 子系统。



## 基本概念

memblock 足够简单，简单到将所有的物理内存一视同仁，但实际上鉴于内核亦或是硬件的一些特性，内核如果想要更高的效率和利用率，比如对内存管理工作进行细化。在了解 buddy 所做的这些工作之前，我们需要先了解几个概念。



### 内存模型

内存模型这个概念其实是有些抽象的，没办法，对应的英文就是 memory model，这是针对整体物理内存管理的策略，包括两个方面：

* 每个页面使用 struct page 进行描述，所有页面对应的 struct page 结构应该如何被组织
* struct page 结构和对应的物理页面之间如何建立映射，通过 struct page 找到物理页面是简单的，大不了使用一个结构成员指针即可，但是如何通过物理页面索引到 struct page 结构呢？要知道物理页面中是不会保存任何管理数据的，唯一可用的只有一个基地址。

内核中的内存模型分为三种：

* FLATMEM：平坦内存模型
* DISCONTIGMEM：非连续内存模型
* SPARSEMEM：稀疏内存模型

软件总归是基于硬件进行抽象，内核使用哪种内存模型取决于系统的物理内存是如何分布的，在嵌入式系统中，物理内存通常是连续的，这种情况下的物理内存管理是最简单的，平坦内存模型适用于这种情况。

在内核中，既然要对每个物理页面进行管理，那么自然是需要使用特定的数据结构对其进行描述，记录页面的相关信息，内核中使用 struct page 结构来描述一个页面，在平坦内存模型中，所有的页面对应的 struct page 结构可以存放在一片连续的内存中。

这种结构带来的好处在于：物理页帧(号)与 struct page 之间的关联实现非常简单，这种关联在于通过一方可以快速地索引到另一方，连续的内存可以直接使用偏移地址获得对方的地址。 

实际的情况也可能并不理想，系统中连接的物理内存可能并不是连续的，被分为一块或者多块，这种情况在 numa 架构中多见，页面依旧使用 struct page 来表示，但是如果还是按照原来的方案(连续内存)来保存所有物理内存的 struct page，这种物理内存上的空洞必定会带来内存空洞处对应的 struct page 是没有对应物理页面的，也就是会造成内存的浪费(当前 arm32 平台上一个 struct page 占用 32 字节内存，1G 内存的空洞就导致浪费 8M 空间)，为了避免这种浪费，非连续内存模型被提出，顾名思义，这种内存模型用来管理非连续的物理内存。理想的情况是，在 numa 架构中，每个节点中的物理内存是连续的，这样对每个 numa 节点使用独立的数据结构来描述，一方面可以节省内存，另一方面同样可以做到 struct page 与物理页面之间的简单映射，但是需要增加判断内存所在节点的成本，同时这种方案对内存的热插拔支持性不好，如果单节点中出现非连续的内存(尽管很少见)，则更不好处理，因此，DISCONTIGMEM 只在内核中存在较短的时间。

一个更好的方案是稀疏内存模型(SPARSEMEM)，这是目前最受欢迎的内存模型，64位平台中通常将这种内存模型设为默认值，内存模型是否合适依旧是那两个判断标准，一是是否在支持非连续内存的同时不造成内存的过多浪费，二是该内存模型是否能很好地处理 struct page 与物理页面之间的映射，显然 SPARSEMEM 是做到了这两点。

SPARSEMEM 在内存的描述中使用了 section 的概念(注意这里的section 和 MMU 的 section 映射不是同一个概念)，section 的大小由体系结构决定，由原来对整块内存的操作变成对多个内存 section 的操作，分配 struct page 相关的操作更加灵活，可以做到按需分配。

为了在物理页帧号 pfh 和 strcut page之间进行高效转换，物理页帧号 pfn 的几个高位用于索引sections数组，解决了从物理页面到 struct page 的映射，另一方向上，段号被编码在 struct page 中，由 struct page 到物理页面的映射也就解决了。

在实际的应用中，第二种内存模型基本上已经被抛弃了，在较为复杂的内存系统中(存在非连续内存的系统)，通常使用SPARSEMEM ，而对于只提供连续内存的系统，当然是要使用 FLATMEM，简单且快捷。 

在我当前使用的平台上，使用的是 FLATMEM，大部分嵌入式平台不支持 numa 或者内存热插拔等比较高级的内存特性。

在内核的初始化阶段，将会一次性为所有的物理页面 struct page 结构体，占用连续的内存，基地址保存在全局变量 struct page* mem_map 中，因此，struct page 和屋里页帧号之间的映射实现为：

```c++
#define __page_to_pfn(page)	((unsigned long)((page) - mem_map) + \
				 ARCH_PFN_OFFSET)
#define __pfn_to_page(pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
```

实现非常简单，从原理上来说，也就是物理页面相对于物理内存基地址的偏移等于 struct page 结构相对 mem_map 基地址的偏移，基于这个进行双向的映射。

而 DISCONTIGMEM 和 SPARSEMEM  类型的映射，同样可以参考 include/asm-generic/memory_model.h 头文件。  

 

### 内存域 - zone

内核中所有内存并不是被一视同仁的，它们会根据硬件的特性被划分成不同的内存域，对于 32 位系统而言，内存区域的限制将会更加明显。 

目前内核中支持的内存域如下：

```c++
enum zone_type {
#ifdef CONFIG_ZONE_DMA
	ZONE_DMA,
#endif
#ifdef CONFIG_ZONE_DMA32
	ZONE_DMA32,
#endif
	ZONE_NORMAL,
#ifdef CONFIG_HIGHMEM
	ZONE_HIGHMEM,
#endif
	ZONE_MOVABLE,
#ifdef CONFIG_ZONE_DEVICE
	ZONE_DEVICE,
#endif
	__MAX_NR_ZONES
};
```

这是一个枚举列表，包含 6 个有效成员，最后一个 __MAX_NR_ZONES 用于记录 zone 的最大数量，用于在遍历的时候作为结束判断条件，或者定义数组时设置成员数量，在内核中这种手法是很常见的。

从各种条件宏不难看出，上面所列出的所有 zone 除了 ZONE_NORMAL 和 ZONE_MOVABLE 之外，其它的 zone 都是可配置的，主要和硬件相关。 

ZONE_DMA：在



### NUMA 节点

尽管本系列文章并不涉及到 numa 系统的分析，同时，内核针对 numa 和 非 numa 系统使用统一的接口，在分析内存时还是不能忽略这个概念。

在内核中，buddy 子系统会将所有机器抽象为 numa 架构，非 numa 系统被内核视为单节点 numa 系统，这样就可以实现接口的统一。

因此，不难推出，一个 numa 节点就是一个完整的内存管理区域，每个 numa 节点包含一个 struct pglist_data 结构，同样是以 pgdat  -> zones  -> pages 的树形结构来管理一片完整的物理内存。 











zone

为 struct page 分配内存



 内存模型：https://zhuanlan.zhihu.com/p/220068494