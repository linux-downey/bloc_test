# buddy 子系统 - 初始化

在内核的内存初始化阶段，memblock 在完成了一些最基本的物理内存信息收集以及必要的内存分配之后，就需要着手开始向 buddy 系统进行迁移了，毕竟 buddy 子系统越早启动，内核就越早进入到内存管理的正轨。

但是这个过程并不是一步就位的，内存所涉及到的方方面面本身是非常复杂的，尽管 buddy 系统已经足够简单，那也只是相对其它的内存管理器而言，本章着重于分析 buddy 子系统的初始化的早期部分，也就是一些准备工作。 



## 基本概念

memblock 足够简单，简单到将所有的物理内存一视同仁，但实际上鉴于内核亦或是硬件的一些特性，内核如果想要更高的效率和利用率，比如对内存管理工作进行细化。在了解 buddy 所做的这些工作之前，我们需要先了解几个概念。



### 内存模型

内存模型这个概念其实是有些抽象的，没办法，对应的英文就是 memory model，这是针对整体物理内存管理的策略，包括两个方面：

* 每个页面使用 struct page 进行描述，所有页面对应的 struct page 结构应该如何被组织
* struct page 结构和对应的物理页面之间如何建立映射，通过 struct page 找到物理页面是简单的，大不了使用一个结构成员指针即可，但是如何通过物理页面索引到 struct page 结构呢？要知道物理页面中是不会保存任何管理数据的，唯一可用的只有一个基地址。

内核中的内存模型分为三种：

* FLATMEM：平坦内存模型
* DISCONTIGMEM：非连续内存模型
* SPARSEMEM：稀疏内存模型

软件总归是基于硬件进行抽象，内核使用哪种内存模型取决于系统的物理内存是如何分布的，在嵌入式系统中，物理内存通常是连续的，这种情况下的物理内存管理是最简单的，平坦内存模型适用于这种情况。

在内核中，既然要对每个物理页面进行管理，那么自然是需要使用特定的数据结构对其进行描述，记录页面的相关信息，内核中使用 struct page 结构来描述一个页面，在平坦内存模型中，所有的页面对应的 struct page 结构可以存放在一片连续的内存中。

这种结构带来的好处在于：物理页帧(号)与 struct page 之间的关联实现非常简单，这种关联在于通过一方可以快速地索引到另一方，连续的内存可以直接使用偏移地址获得对方的地址。 

实际的情况也可能并不理想，系统中连接的物理内存可能并不是连续的，被分为一块或者多块，这种情况在 numa 架构中多见，页面依旧使用 struct page 来表示，但是如果还是按照原来的方案(连续内存)来保存所有物理内存的 struct page，这种物理内存上的空洞必定会带来内存空洞处对应的 struct page 是没有对应物理页面的，也就是会造成内存的浪费(当前 arm32 平台上一个 struct page 占用 32 字节内存，1G 内存的空洞就导致浪费 8M 空间)，为了避免这种浪费，非连续内存模型被提出，顾名思义，这种内存模型用来管理非连续的物理内存。理想的情况是，在 numa 架构中，每个节点中的物理内存是连续的，这样对每个 numa 节点使用独立的数据结构来描述，一方面可以节省内存，另一方面同样可以做到 struct page 与物理页面之间的简单映射，但是需要增加判断内存所在节点的成本，同时这种方案对内存的热插拔支持性不好，如果单节点中出现非连续的内存(尽管很少见)，则更不好处理，因此，DISCONTIGMEM 只在内核中存在较短的时间。

一个更好的方案是稀疏内存模型(SPARSEMEM)，这是目前最受欢迎的内存模型，64位平台中通常将这种内存模型设为默认值，内存模型是否合适依旧是那两个判断标准，一是是否在支持非连续内存的同时不造成内存的过多浪费，二是该内存模型是否能很好地处理 struct page 与物理页面之间的映射，显然 SPARSEMEM 是做到了这两点。

SPARSEMEM 在内存的描述中使用了 section 的概念(注意这里的section 和 MMU 的 section 映射不是同一个概念)，section 的大小由体系结构决定，由原来对整块内存的操作变成对多个内存 section 的操作，分配 struct page 相关的操作更加灵活，可以做到按需分配。

为了在物理页帧号 pfh 和 strcut page之间进行高效转换，物理页帧号 pfn 的几个高位用于索引sections数组，解决了从物理页面到 struct page 的映射，另一方向上，段号被编码在 struct page 中，由 struct page 到物理页面的映射也就解决了。

在实际的应用中，第二种内存模型基本上已经被抛弃了，在较为复杂的内存系统中(存在非连续内存的系统)，通常使用SPARSEMEM ，而对于只提供连续内存的系统，当然是要使用 FLATMEM，简单且快捷。 

在我当前使用的平台上，使用的是 FLATMEM，大部分嵌入式平台不支持 numa 或者内存热插拔等比较高级的内存特性。

在内核的初始化阶段，将会一次性为所有的物理页面 struct page 结构体，占用连续的内存，基地址保存在全局变量 struct page* mem_map 中，因此，struct page 和屋里页帧号之间的映射实现为：

```c++
#define __page_to_pfn(page)	((unsigned long)((page) - mem_map) + \
				 ARCH_PFN_OFFSET)
#define __pfn_to_page(pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
```

实现非常简单，从原理上来说，也就是物理页面相对于物理内存基地址的偏移等于 struct page 结构相对 mem_map 基地址的偏移，基于这个进行双向的映射。

而 DISCONTIGMEM 和 SPARSEMEM  类型的映射，同样可以参考 include/asm-generic/memory_model.h 头文件。  

 

### 内存域 - zone

内核中所有内存并不是被一视同仁的，它们会根据硬件的特性被划分成不同的内存域，对于 32 位系统而言，内存区域的限制将会更加明显。 

目前内核中支持的内存域如下：

```c++
enum zone_type {
#ifdef CONFIG_ZONE_DMA
	ZONE_DMA,
#endif
#ifdef CONFIG_ZONE_DMA32
	ZONE_DMA32,
#endif
	ZONE_NORMAL,
#ifdef CONFIG_HIGHMEM
	ZONE_HIGHMEM,
#endif
	ZONE_MOVABLE,
#ifdef CONFIG_ZONE_DEVICE
	ZONE_DEVICE,
#endif
	__MAX_NR_ZONES
};
```

这是一个枚举列表，包含 6 个有效成员，最后一个 __MAX_NR_ZONES 用于记录 zone 的最大数量，用于在遍历的时候作为结束判断条件，或者定义数组时设置成员数量，在内核中这种手法是很常见的。

从各种条件宏不难看出，上面所列出的所有 zone 除了 ZONE_NORMAL 和 ZONE_MOVABLE 之外，其它的 zone 都是可配置的，主要和硬件相关。 

* ZONE_DMA：在一个完整的硬件系统中，CPU 并不是系统总线的掌控者，DMA 同样可以操作系统总线来访问内存，其主要的作用就是协助 CPU 完成一些预定义的数据拷贝工作，让 CPU 腾出手来做其它更多的事，从而提高程序执行效率。
  在一个 32 位的系统中，CPU 延伸出的系统总线宽度为 32 位(非地址扩展的情况)，但是 DMA 硬件控制器并不一定满足 32 位的地址总线宽度，比如 x86 架构中的某些 DMA 设置只支持 24 位的地址线，这就意味着该 DMA 只能访问 16MB 的内存空间。
  同时，鉴于 linux 需要兼容这些硬件设备，软件上就需要统一做一些限制：在 DMA 控制器总线宽度不满足的情况下，分配给 DMA 的内存不能超过 16MB 的限制，否则会出现 DMA 无法正常操作内存的情况，因此内核中将内核基地址的 0-16MB 内存划分成 DMA_ZONE，在申请 DMA 内存时从这个区中申请才是安全的。
  随着硬件的发展，或者在非 x86 的其它架构中，DMA 只支持 24 位地址线的情况并不多见，因此，DMA_ZONE 在内核中越来越少见，取决于 CONFIG_ZONE_DMA 是否被配置，当然，最根本的还是取决于实际硬件的情况。
*  ZONE_DMA32：和 ZONE_DMA 相类似的概念，这里考虑的是 64 位系统的情况，如果系统中使用的 DMA 硬件控制器只支持 32 位的地址线，那么也就需要将 DMA 内存申请控制在内核地址开始的 0-4G 空间内，也就在 64 位系统中划分出  ZONE_DMA32 区。
* ZONE_NORMAL：正常的内存区域，也是常用的内存管理区，这里的 NORMAL 是相对于其它 zone 的概念，因为其它 zone 都有些特殊，比如 DMA 需要连续物理内存，HIGHMEM 区不支持持久映射。 
* ZONE_HIGHMEM：高端内存，对于 linux 要了解的一个基本概念为：所有硬件资源都由内核管理，物理内存自然也不例外，从用户空间的角度来说，看起来是对于内存的操作，但是实际上所有用户使用的内存空间都是先由内核分配，再执行映射的。
  默认的配置下，32位系统中内核和用户空间的占比为 1：3，也就是内核只占 1G 的线性地址，如果此时系统中存在 2G 的物理内存，那么在使用内存时，内核无法完整地映射所有的物理内存，因为一个页面如果需要被使用，它必须要被映射到内核区域中，需要注意的是，这里说的是无法直接映射，并不是无法管理，内核中对每个页面使用 struct page 来描述。
  因此，在物理内存无法直接全部映射的情况下，将一部分区域直接线性映射到内核中，而另一部分区域留出来，执行动态地映射，动态映射也就是当那些多出来的无法执行直接映射的内存区域需要使用时，临时建立页面到内核中的映射，当不用时，就解除映射，以这种方式来动态地管理多余的内存，而动态映射区也就被划分为 ZONE_HIGHMEM。
  在 32 位内核中，高端内存与低端内存的分界点一般配置为 896MB 或者 768MB 处。
  对于 64 位系统而言，不存在这个问题，毕竟内核的线性空间可以划分为非常大(TB级)的地址，不会存在物理地址大于内核线性地址的问题，因此在 64 位系统中，所有物理内存都可以实现线性映射。
* ZONE_MOVABLE：可迁移的页面，内存和磁盘不一样，对于磁盘而言，数据可以随意地迁移来迁移去，但是对于某些内存就不行，程序代码通常对地址具有依赖性的。比如内核镜像部分的代码是不能随意移动的，但是由于虚拟内存机制提供的抽象，用户空间程序是可以移动的，对于用户空间程序而言，本来就不在乎其运行的物理地址在哪里，只需要满足其虚拟地址对应就行，移动它们之后只需要修改以下页表项，重新建立映射即可。
  设置这个区域主要是针对内核的内存回收以及碎片处理。
* ZONE_DEVICE：与特定设备相关的内存区域，不能被普通的内存操作接口访问



内核中以 ZONE 为单位，对物理内存进行管理，不同的 ZONE 自然有不同的属性，也针对不同的用途，比如在 imx6ull 上，配置了三个 ZONE：ZONE_NORMAL、ZONE_HIGHMEM、ZONE_MOVABLE，其中 ZONE_NORMAL 区域的内存执行线性映射，DMA 申请的内存也属于这个区域。ZONE_HIGHMEM 用于动态映射。



### NUMA 节点

尽管本系列文章并不涉及到 numa 系统的分析，同时，内核针对 numa 和 非 numa 系统使用统一的接口，在分析内存时还是不能忽略这个概念。

在内核中，buddy 子系统会将所有机器抽象为 numa 架构，非 numa 系统被内核视为单节点 numa 系统，这样就可以实现接口的统一。

因此，不难推出，一个 numa 节点就是一个完整的内存管理区域，每个 numa 节点包含一个 struct pglist_data 结构，同样是以 pgdat  -> zones  -> pages 的树形结构来管理一片完整的物理内存。 



## buddy 启动前期



### buddy 初始化背景

memblock 作为内核早期的内存管理器，有几个重要的阶段：

* 在内核中被静态定义，整个内存管理器由 memory 和 reserved 两个数组来管理物理内存

* 扫描设备树，获取系统中所有物理内存信息
* 为内核镜像、dtb、设备树中指定内存等区域设置保留空间，并针对一些必要的部分分配物理内存
* 为所有物理内存建立页表，不过建立页表的工作并不完全算 memblock 来做的，它只是提供相应的内存信息，总之，在此之后，系统中所提供的所有物理内存及其属性都已经确定，线性映射区可以访问了。

在这种情况下，buddy 子系统已经具备了启动的环境，memblock 开始慢慢地将内存管理的重任交接给 buddy 子系统。



## buddy 初始化准备阶段

在 paging_init 函数中建立完页表之后，在 paging_init 函数的后半段执行 bootmem_init，开始向 buddy 系统靠近。  



















 内存模型：https://zhuanlan.zhihu.com/p/220068494