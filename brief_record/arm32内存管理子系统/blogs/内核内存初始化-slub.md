# 内核内存初始化 - slub 分配器

内核中 buddy 子系统管理内存的最小单位是页，页大小通常配置为 4K，出于效率的考虑，如果使用更小的页将会带来更大的管理成本，对于整个物理内存来说，尽管 4k 的粒度已经分得很小，但是对于普通程序来说，操作内存的最小单位为字节，通常都是几十到几百的内存申请请求，而直接使用一个页是非常浪费的。

最容易想到的解决方案就是对一个页面的复用，多个对象共享一个页面，一种方案是可以将 buddy 子系统的思路移植到页内分配，也就是将 4K 页内内存多块内存，以 2 次幂的大小分别链入不同链表，当用户产生内存分配的需求时(以字节为单位)，找到最近的一个满足条件的块给用户，如果没有，就将更大的内存块一分为二，再执行分配，和 buddy 子系统针对页面的分配思路一致。

尽管 buddy 分配算法在页面级的内存分配非常简单且高效，但是它并不适合用于页内内存分配。

一方面，内核中采用 4K 页面大小本就是基于内存利用率的考虑，大页面会造成在使用时页面空间的大量剩余，从而导致内存浪费，而更小的页面则导致页面管理数据量的急剧提升，单个页面的浪费问题解决了，但是页面管理上浪费了更多的内存。从设计上来说，buddy 子系统的管理成本并不低，数据结构相对复杂，为了保存 buddy 的数据结构就需要使用掉相当一部分内存，因此，需要管理成本更低的页内分配器。 

另一方面，以 2 次幂为一阶的内存分配方式在分配小内存时问题不大，假设如果想分配 130 字节的内存就不得不使用 256 字节的内存块，总感觉还是浪费严重，但是出于管理成本的考虑，又不能切分太多的块，因此，需要利用率更高的页内内存分配方式。 

针对这些页内内存分配需求，linux 内核采用了 slab 分配器， slab算法是1994年开发出来的并首先用于sun microsystem solaris 2.4 操作系统，后来被引入到 linux 中，slab 在一定程度上优化了内存利用率问题，其管理成本相对在可接受范围内。 

尽管slab分配器对许多可能的工作负荷都工作良好，但也有一些情形，它无法提供最优性能。slab 分配器的弊端在于其用来管理的元数据太多，同时复杂性也相对较高，在嵌入式系统或者配备大量内存的大型系统中不太适用，对于资源受限的嵌入式系统而言，需要更精简的内存管理器以占用更少的资源。而对于大型系统，slab 所使用的数据结构可能就需要占用数 G 的内存空间，这自然也难以接受。

因此，针对这些需求，在内核版本 2.6 的开发期间，增加了两种 slab 分配器的替代品：slub 和 slob。

* slob 分配器是针对资源受限型设备的优化，代码量小，管理数据所占内存得到优化，尽管在分配效率上略差于 slab，不过也是可以接受的。

* slub分配器通过将页帧打包为组，并通过struct page中未使用的字段来管理这些组，试图最

  小化所需的内存开销，同时简化了一些冗余的管理结构

随着 slub 的逐步优化，逐渐被内核开发者接受，同时也逐渐用来替换 slab 作为内核默认内存管理器，而随着硬件资源的发展，slob 的出场率并不高，至少在我所接触的平台中，不论是嵌入式平台 imx6ull、beagle bone、imx8，还是 PC 平台(基于 x86_64 的 ubuntu18)，都选择 slub 分配器作为页内分配器，本章内容也是基于 slub 分配器的讨论。

得益于内核优秀的抽象化设计，这三种内存分配器在接口上是兼容的，使用者只需要使用 kmalloc 申请内存，获得内核返回的内存并使用，并不需要关心底层的内存分配是如何实现的。内核开发者通过内核配置选择内存分配器。



## 基本术语

在下文的讨论中，使用的几个名词需要说明，以免混淆：

* slab：



## slub 概览

同时，由于 slub 是基于 slab 分配器衍生而来的，多数接口兼容，因此 slub 的实现代码中保留着一些 slab 分配器的概念，比如对于 slub 中的缓存块组，依旧称之为 slab(不是指 slab 分配器，而是缓存块组)。

那么，回到最核心的问题，slub 分配算法是如何工作的呢？

*  slub 基于 buddy 子系统，slub 分配器所占用的页面由 buddy 子系统分配而来，在特定时候也将释放到 buddy 子系统中
* slub 分配器基于一种缓存机制，是针对同一类型对象的缓存，也就是完整的页面会根据目标对象分成多个缓存块，每次分配内存就返回其中一块。这种由多个相同大小的块组成的页面就是一个 slab(注意这里所说的一个 slab 其实是一个缓存块组)，其实页内分配器其实并不是严谨的说法，只是为了表达 slub 是针对小内存的分配，这种内存分配是完全可能跨页的。因此，一个 slab 可能包含多个页面，只是通常对于比较小的缓存对象是一个页面。
* 系统默认创建了特定 size 的 slab 缓存块组，默认创建了从 8 - 192 bytes(以 8 为间隔) 的 slab，基本覆盖了小内存的申请需求，且不会造成明显的内存浪费(一个缓存对象最多浪费 7 个 bytes)，同时，最大的特点是：内存申请者可以通过目标对象的大小自定义 slab，假设一个内核中出场率很高的结构需要频繁申请释放内存，占用 260 bytes，可以使用 kmem_cache_create 函数为其创建一个专用的 slab，该 slab 中包含连续且多个刚好为 260  bytes 的空闲缓存块，后续针对该对象的申请释放不会浪费哪怕一个字节的内存(在某些情况下可能会浪费一些对齐字节)，解决了内存在实际使用中利用率的问题。
* 内存利用率还取决于另一个因素：管理成本，也就是 slab 分配器本身要占用的空间。slub 中对一个 slab 的管理是基于链式结构，一方面，因为一个 slab 中缓存都是同一对象，因此不需要过于复杂的数据结构来管理，另一方面，slub 分配器巧妙地使用空闲页面来记录缓存对象，省去了一部分内存的开销。

slub 的整体框架如下：

![image-20210328225747735](buddy与slub的整体框架.png)



每个 slab 占用连续的一个或多个物理页面，所有页面都是通过 buddy 子系统分配而来，在所有的 slab(slub 缓存组) 中，有一些比较特殊的 slab 成员，比如最典型的就是用于管理 slab 结构的 slab，说起来有点绕。

通俗来讲，假设内存需要调用 kmem_cache_create 为某个数据结构创建一个新 slab 时，首先需要做的并不是为 该结构申请内存空间，而是需要先申请一个 kmem_cache 结构，之后再针对目标对象申请对应的 slab 空间(就像组建一个小队需要先找一个队长来管理)，该数据结构用来管理新建的 slab，kmem_cache 占用较小的内存空间，也需要通过 slab 来管理，因此 kmem_cache_slab 中保存的就是所有的 kmem_cache 管理数据(该小队中全是队长)，从而体现出这个 slab 的特殊性。 

一个 slub 对象并不一定只包含一个 slab，当分配的一个 slab 用完时，会再次向 buddy 申请页面，分配新的 slub 缓存块组。

对于 slab 的管理来说，最基本的就是了解当前 slab 中哪些缓存块是空闲的，哪些是被占用的，slub 使用的方式是链式结构，同时只记录空闲缓存块，也就是 kmem_cache 中保存 slab 中第一个空闲缓存块的位置，而第一个空闲缓存块中保存下一个空闲缓存块的位置，依次类推，这种利用没有分配出去的内存来记录下一节点的方式节约了不少内存。

上面只是粗略地介绍了一下 slub 的框架与基本概念，接下来我们再深入到 slub 中，看看其对应的具体实现。



## slub 数据结构

按照惯例，在简单介绍基本概念之后，我们先要来看看该组件的数据结构，从数据结构入手，了解 slub 的实现。 



### kmem_cache

kmem_cache 是 slub 中的核心数据结构，所有 slab 的管理都由  struct kmem_cache 结构执行：

```c++
struct kmem_cache {
	struct kmem_cache_cpu __percpu *cpu_slab;
	unsigned long flags;
	unsigned long min_partial;
	int size;		
	int object_size;	
	int offset;		
	int cpu_partial;	
	struct kmem_cache_order_objects oo;
	struct kmem_cache_order_objects max;
	struct kmem_cache_order_objects min;
	gfp_t allocflags;	
	int refcount;		
	void (*ctor)(void *);
	int inuse;		
	int align;		
	int reserved;		
	const char *name;	
	struct list_head list;	
	int red_left_pad;	
	struct kobject kobj;	
	struct kmem_cache_node *node[MAX_NUMNODES];
};
```

* cpu_slab：percpu 类型的 slab，和 buddy 子系统中的 pcp page 一样，percpu 类型的 slab 是为了更好地利用 CPU 缓存，降低缓存失效的发生，该结构的定义如下:

  ```c++
  struct kmem_cache_cpu {
  	void **freelist;	
  	unsigned long tid;	
  	struct page *page;	
  	struct page *partial;	
  };
  ```

  * freelist：指向 slab 中第一个空闲的缓存块
  * tid：这是一个用作校验的字段，主要用来判断 percpu 的操作过程中进程是否切换到另一个 cpu 上，从而导致 CPU 与 cpu_slab 之间的不匹配，如果发生这种情况需要重新操作。
  * page：用于索引当前 percpu 类型 slab 所占用 page 或 pages，如果是多个 page，后续的 page 通过 next 指针相连。
  * partial：未满的 slab 缓存块组，被当成备用的分配列表，在分配的过程中自然是将一个 slab 用完之后再申请下一个，但是释放的时候可就没这么规律了，如果释放的是已经分配完的 slab，就会导致该 slab 需要被重新管理，也就放在该 partial 链表中，当前的 slab 分配完之后，会从 partial 中分配。 

* flags：slab 相关的标志位

* node[MAX_NUMNODES]：这原本是最后一个成员，因为它涉及到 slub 的基本实现机制，因此提到前面来讲。
  在 slub 中，每个缓存对象通常会维护两重 slab，一重就是上面提到的 cpu_slab，一重就是当前 struct kmem_cache_node 类型的 slab，后缀 node 表示该 slab 是 per node(NUMA node) 的，MAX_NUMNODES 就是 NUMA node 节点的数量，在非 NUMA 系统中就代表是所有CPU 共享的，在缓存块分配和释放的时候，自然是优先处理 percpu 类型的 slab，毕竟分配效率高，但是也不能无限制地扩展 percpu 类型的 slab，因此就会使用到 per node 的 slab，其实这种策略也相当于是做了一层缓存，在 per node 类型的 slab 之后，就是 buddy 子系统，当缓存对象对应的空闲 slab 过多时，需要将页面返回给 buddy，如果用完了，就得从 buddy 申请，以维持一种动态的平衡。 

* min_partial：(kmem_cache_node) node  上存在的 slab 数量的下限值，当对象缓存块被释放时，

