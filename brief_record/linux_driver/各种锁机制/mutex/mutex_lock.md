# mutex lock
spin lock 是在 linux 内核中实现的一种忙等机制，本质上是对 mutex lock 的一种优化，对于那些执行时间非常短的临界区而言，没有必要让进程进入休眠，因为进程切换的开销可能远大于临界区执行时间，因此就设计了 spinlock 的机制代替 mutex lock 来提升锁的性能。  

而对于更小的共享对象操作，比如单个 int、long 类型的共享变量的操作，则通过原子操作来实现，这是架构相关的。   

mutex lock 本身在操作系统领域是一个通用的概念，不仅仅是 linux 中，对于其它的操作系统同样提供了 mutex lock，其操作接口基本上是一样的：使用互斥的临界区保护共享对象，当临界区被其它进程占用时，尝试进入的临界区的进程进入休眠，等待占用者退出临界区，这个占用和退出的过程对应 mutex 的加锁和解锁。  

对于 mutex 而言，尽管各操作系统之间对外的接口几乎一致，但是其具体的实现一般不同，本章节就是讨论 linux 内核中 mutex lock 的实现。    


# 互斥锁
抛开 linux 的实现，从通用的角度来看 spinlock 和 mutex lock 这种二义性的锁，它们的基本特性也比较简单：
1、加锁和解锁由同一个进(线)程完成
2、使用一个状态标志，通过这个标志来控制、判断加解锁
3、实现排队机制，作为同样等待的进程，先进入等待的先获取锁

通过这三点，可以发现通用的二义性互斥锁的实现其实是比较简单的，使用一个结构体来描述所有成员，结构体的成员包括：
* 当前进(线)程识别号，用于在唤醒时找到对应的进(线)程，对于 spin lock，并不需要这个，因为 spin lock 在加解锁的过程中不允许切换进程(是否允许中断依赖于使用的接口)
* 加解锁状态标志，加解锁主要就是操作这个标志来实现，这里面有一个值得注意的问题是：对这个标志的操作必须是原子化的，因为这个标志也会产生竞态，通常这种原子化由平台实现。  
* 排队结构体，通常是链表，也可以是其它的数据结构。和加解锁状态标志一样存在一个问题，这个结构体同样会产生竞争问题，需要通过特定的方式解决，比如禁止中断、或者使用其它同步机制来保证竞态下的数据安全。   

在互斥锁中，spin lock 是 linux 内核的实现，而 mutex lock 几乎是通用的，它们之间的区别也非常明显，就是是否允许睡眠(当然还有其它)。 

如果你看一个实时操作系统的互斥锁实现，大体与上面描述的相差不大，但是在 linux 内核中的实现，要显得复杂很多，一方面是因为 linux 中的并行化环境更为复杂，包括中断、中断下半部、内核抢占以及非常棘手的多核环境。另一方面，随着 Linux 的发展，对执行效率提出了更高的要求，在不断地优化中自然带来了更大的复杂性。  

不过话说回来，不管 mutex 处于多复杂的环境，linux 内核对 mutex lock 如何优化，始终离不开它的那三个特性，只是实现的细节变得更加复杂，这一章我们就来看看内核中的 mutex lock 是如何实现的。    


## mutex 数据结构
和内核中其它组件一样，数据结构基本就反映了该组件的大部分信息，mutex lock 对应 struct mutex 结构：

```C++
struct mutex {
	atomic_long_t		owner;
	spinlock_t		wait_lock;
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
	struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
	struct list_head	wait_list;
#ifdef CONFIG_DEBUG_MUTEXES
	void			*magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
```
struct mutex 数据成员中，CONFIG_DEBUG_MUTEXES 和 CONFIG_DEBUG_LOCK_ALLOC 这两个宏属于 debug 相关的，本章节不做分析。其它的主要成员主要是四个：owner，wait_lock，osq，wait_list。  

参照我们上面对 mutex lock 基本特性的分析，从字面上可以看出：owner 用于记录进程信息，而 wait_list 链表用于实现等待进程的排队，但是好像并没有看到专门用于记录锁状态的字段，同时还多了两个字段：一个 spinlock 和一个看不懂的 struct optimistic_spin_queue osq 字段，这里面有什么玄机呢？  

首先，我们暂时抛开 mutex 数据结构，来考虑一个问题：在内核中使用 mutex 的时候，获取不到锁的就一定要陷入睡眠吗？实际上并不是，在一个进程尝试获取锁时，如果发现占用当前锁的进程正在另一个 CPU 上的临界区运行，就可以乐观地假设该进程会很快地运行完，这种情况下不需要让进程进行休眠，在多核环境下，尤其是锁竞争比较激烈的情况下，这一项优化有非常好的效果，毕竟执行两次调度的时间肯定大于spin 需要的时间。  

linux 内核中对于 mutex 最大的优化就是这一点，这种优化并不是免费的午餐，它同时也会带来一些问题，同时引入了更多的复杂性，基于这一点我们再进入到 mutex 的具体实现。 

## 数据结构成员
在回过头来看 mutex 的数据成员，通过数据成员对 mutex 的实现进行简单的概括：

### owner
owner 是一个 task_struct 成员，保存了进程描述符指针值，记录一个锁的属主，用于实现 mutex 的第一个特性：加锁和解锁由同一个进(线)程完成。   

同时，owner 的作用不仅于此，它还被用来控制加解锁，如果 owner 不等于 0，表示当前锁已经被占用，反之，则表示该锁未被占用。   

而且，因为进程描述符指针总是 8 字节对齐的，即 owner 的 bit0～bit2 恒为 0，内核同时使用了这三位作为标志位，在加解锁过程中记录一些标志信息。  

也就是说，一个 owner 字段，巧妙地记录了三种信息，对于 mutex 这种非常常用的组件而言，节省哪怕一个 byte 的内存都是非常有意义的。

### wait_lock
wait_lock 是一个自旋锁，mutex 主要用来保证链表操作的安全性，因为排队链表同样会带来竞争问题。  

### osq
osq 的数据类型为：struct optimistic_spin_queue，这个数据类型并不常见。在上文中说到，当一个进程尝试获取锁但获取不到时，它会先判断当前持有锁的进程是否正在另一个 CPU 上运行，如果是，它会原地等待。 



