# linux sched_domain 的初始化
在前面的章节中，介绍了 CPU 拓扑结构以及 CPU 的初始化处理，CPU 的拓扑结构被保存在 cpu_topology 这个结构体数组中，包括 CPU 对应的 cluster_id,core_id 等，对于内核而言，CPU 和内存一样，也被看成一项硬件资源，它被动地接收调度器的安排，负责从内存中取出指令、执行指令、必要的时候将执行结果写回到内存。  

在单核系统中，只有一个逻辑 CPU，针对 CPU 的优化就是在有工作需要处理时尽量地不让它停下来，比如在 CPU 等待资源的时候切换到其它工作。在多核中的逻辑也是一样的，比如在一个简单的双核系统中，目标就是尽量地让两个核都不要停下来，除了单核下的那些优化方法之外，还涉及到另一个问题：如何让工作平均地分配到每个核上。这里所说的平均其实也是相对的，因为在异构多核下，核与核之间的功能是有差别的，绝对的平均反而不适合。

系统中程序的最小执行单位为线程，也就是说调度器会将线程或者进程分配到 CPU 上执行，实际上，一个进程从诞生开始，通常都会在睡眠与唤醒之间来回切换，调度器无法预测哪些 CPU 上的进程将会睡眠而什么时候会被唤醒，可能某个 CPU 上的进程已经全部陷入睡眠而其它 CPU 上不堪重负，因此就涉及到一个优化问题：CPU 之间任务的负载均衡，也就是动态地将繁忙 CPU 上的任务迁移到相对空闲的 CPU 上。  

这个迁移的过程涉及到几个问题：



* 什么时候需要迁移？迁移发生在某些特定的场景下，比如 tick 中断、CPU 空闲时。 
* 如何确定哪个 CPU 繁忙而哪个 CPU 空闲？这个统计由调度器完成，调度器可以实时地记录 CPU 对应就绪队列上的负载。  
* 如何确定需要被迁移的任务？找到最繁忙的 CPU 的就绪队列和最空闲的就绪队列，从就绪队列上找到需要迁移的进程。
* 如何进行迁移？一旦确定了要迁移的任务，就直接将任务从源 CPU 的就绪队列上摘下来，放到目标就绪队列上。    

从理论上来说，这三点都是比较简单的，但是对于实际的实现，又涉及到很多非常繁琐的细节处理，比如繁忙和空闲的程度要达到一个什么样的比例才需要迁移？哪些进程不能被迁移？等等.  

在本章中，我们将重点讨论上文中的第一三四点，即确定迁移的进程以及迁移的具体操作，对于第二点 CPU 负载的计算并不详细讨论，这并不是进程迁移的重点。  


## 什么时候执行负载均衡?
负载均衡的触发实际上是非常频繁的，在 tick 中断中会尝试执行负载均衡，也就是内核会周期性地检查 CPU 之间的进程间是否平衡，tick 中断的周期由全局变量 HZ 决定，通常是 10ms 或者 4ms，对应的源码如下：

```c++
void scheduler_tick(void)
{
	...
	trigger_load_balance(rq);
}

void trigger_load_balance(struct rq *rq)
{
	if (time_after_eq(jiffies, rq->next_balance))
		raise_softirq(SCHED_SOFTIRQ);
}

```
执行负载均衡这项操作需要保证一定的间隔时间,太过于频繁地执行负载均衡其实是没有什么意义的,因此 CPU 的 rq->next_balance 用来设置下一次执行负载均衡的时间,这个时间是由调度域设置的,可以通过 /proc/sys/kernel/sched_domain/cpux/domainx/min_interval 和 max_interval 查看对应调度域设置的负载均衡最小和最大间隔时间,比如我的 ubuntu18 系统上的时间就是 2ms/4ms.  

当时间达到 rq->next_balance 时,将会触发软中断,执行负载均衡操作.  

同时，在一个 CPU 上没有可执行的进程时，也就是即将调度执行 idle 进程时，同样将触发负载均衡，这并不难理解，CPU 自己没工作需要执行了，自然要去别的 CPU 上看看有没有多余的工作可以转移过来，对应的内核函数为 idle_balance.  

上述的负载均衡执行的是整体上的负载均衡，而下面几种情况则是为即将运行的进程选择合适的 CPU，对系统的负载均衡有所贡献，但是并不算严格的负载均衡：
* 在执行 fork 创建子进程时，子进程被第一次调度运行时，将会选择一个合适的 CPU 执行。 
* 在进程发起 execve 系统调用时，将会选择一个合适的 CPU 执行。 
* 当进程被唤醒时，同样会选择合适的 CPU 执行。 

上面只是提到了负载均衡触发的时机以及对应的接口,其具体的源码实现我们将在后面讨论.   

## CPU 调度域
在 CPU 的负载均衡中有一个非常重要的概念:CPU 的调度域,在前面的章节中(TODO),介绍了 CPU 的拓扑结构以及缓存相关的知识,对于一个复杂的系统而言, CPU 的拓扑结构层次为 NUMA->socket(物理CPU)->cores->SMT,为什么分成这样的层次在该章节中有相应的介绍,从调度层面来看,同一个区域内的逻辑CPU之间迁移任务的开销是最小的,主要是出于缓存的考虑.   

对于同一个核心上的不同硬件线程来说,它们的大多数硬件资源都是共享的,core 内 core 外的所有级别的 cache 自然也是共享的,因此,将一个任务迁移到相同 core 的另一个硬件线程上,成本非常低.  

如果将任务在同一个 socket 中的不同 core 之间迁移,core 内的 cache(L1 为 per core,L2取决于处理器实现)是不共享的,而上层的 unify cache 和内存是共享的,将这些进程进行迁移的时候 core 内的 cache 需要重新填充,有一定的成本.  

实际上,NUMA 结构的实现并不只是一层,因为不想过多讨论 NUMA,因此我把它的概念简化了,它至少是有两部分的:一个 NUMA 系统包含多个 NUMA 节点,而一个 NUMA 节点也可以包含多个 socket,因此,跨 socket 的逻辑 CPU 之间的进程迁移只会共享本地内存或者 unify 内存部分,跨 NUMA 节点的进程迁移,可以想到,只共享最后一层 unify 内存.当然,迁移成本也是逐渐升高的.  

同时,上述所描述的 CPU 拓扑结构其实只是一个通用的概念,具体是什么结构取决于厂商的实现,而实现根据应用场景而定,通常是非常灵活的,比如多层次的大小核,非对称的物理 CPU 等等.我们所讨论的只是一种普遍情况,具体情况还需要参考对应的手册.   

### sched_domain
既然不同的 CPU 层级中进程迁移的成本是有区别的,这种区别自然也会在软件中有所体现,内核中使用 sched_domain 来描述调度域,调度域的抽象和硬件上的层级是类似的,不同的调度域层级中存在父子关系，由 sched_domain 中的 parent 和 child 指针建立父级的索引关系.  

同一个调度域内,所有同层级成员拥有相同的属性,比如一个 socket 中存在多个 core, 在 socket 调度域中,这些 core 共享 L3 cache(以及可能的 L2) 和内存资源,而在 core 所属的调度域内,所包含的硬件线程共享所有 level 的 cache 和内存,划分调度域的作用在于:在执行负载均衡时,以调度域为单位进行进程迁移,优先在调度域内处理,再依次向上处理高层级调度域,可以降低迁移的成本.同时也降低了软件的复杂度.    

### sched_group 
调度域的概念比较好理解,直接参考 CPU 的拓扑结构即可,但是在实际的负载均衡操作中,使用地更多的概念是调度组,有了调度域来区分 CPU 的层级为什么还需要调度组?   

组织了半天语言,发现没法儿通过文字的方式比较精确地表达 sched_domain 和 sched_group 之间的关系,还是通过示例和图来解释吧.  

假设在一个系统中,存在两个 socket,每个 socket 包含 4 个核心,而每个核心包含两个硬件线程,sched_group 和 sched_domain 的关系见下图(TODO):

从图中可以看出，因为包含两个 socket，因此顶层的 domain 包含两个子 domain，每个子 domain 对应一个 group，group 以循环链表的方式组织起来，sched_domain->groups 指向第一个 group。  

每个 group 的 cpumask 字段保存了子 domain 包含的所有 CPU，因此，在 Level3 domain 的 Grp0 包含 CPU0~cpu7 共 8 个逻辑 CPU，而 Grp1 类似。  

在 L2 domain 中，每个 socket 包含四个 core，也就是四个子 domain，由四个组表示，每个组包含两个逻辑 CPU，这里的逻辑 CPU 实际上是一个 core 上的两个硬件线程，依次类推。  

实际上，由于空间的原因，上图只是展示了 domain 与 group 在系统中的总体关系，这并不对应数据结构中记录的结构，具体情况如下：
* **CPU domain 和 group 在内核中对应的数据结构为 sched_domain 和 sched_group，需要注意的是，sched_domain 是 percpu 的，在上图中每个调度域拥有多个子 domain，但实际上每个 CPU 只会记录当前 CPU 所属的调度域信息，也就是在本 CPU 上，每个 domain 只存在一个 parent 和 child，也就是每个 CPU 只保存了和自身相关的 domain 树状结构的某一条分支。再次强调！上图只是从系统的角度描述的调度域与调度组的情况，但是实际上并不存在对应的软件数据结构，这些 sched domain 和 sched group 信息是被每个 CPU 分开记录的。**   
* 在 L1 domain 中，由于空间原因将 Grp 信息省略了，实际上 Grp0/1 和 L2 domain 中一样，只是 L1 domain 中包含两个 group，每个 group 只包含一个逻辑 CPU。   

由此可知,domain 关注的是层次关系,而 group 则是对 CPU 的一种组织,在做系统的负载均衡时,domain 只是限定了负载均衡的区域,而 group 包含了需要处理的目标 CPU 集合,因此负载均衡中寻找最忙的 CPU 的操作通常是这样的:

```c++
for_each_domain(this_cpu, sd) {
	...
	group = find_busiest_group(&env);
	busiest_queue = find_busiest_queue(&env, group);
	...
}
```
从当前调度域向上递归到每个父级调度域,在每个调度域中,找到最忙的组,在组中找到最忙的 runqueue,也就是最忙的 CPU.  

理解了调度域和调度组之间的联系以及工作方式,负载均衡的难点几乎就已经解决了一半.  

## 选择合适的 CPU 执行进程
当一个进程新创建,调用 exec 或者被唤醒时,通常不需要考虑 cache 的问题(唤醒进程可能会有cache的残留),因此这时候将进程迁移到其它 CPU 并没有太多影响,选择合适的 CPU 通过 select_task_rq 函数,该函数会调用对应 sched_class->select_task_rq 回调函数,找到一个合适的 CPU 执行,最主要的影响因素是 CPU 的亲和性设置,其次再是取决于 CPU 的繁忙程度.  

对于经历睡眠然后被唤醒的进程而言,如果睡眠时间较短或者 CPU 并不太繁忙,是有可能该进程对应 CPU 的 cache 还没有被刷掉的,因此,如果是睡眠唤醒,需要考虑这个问题,也就是如果唤醒 CPU 与被唤醒进程之前所在的 CPU 在同一个低层的 domain 内(通常所有 CPU 是在同一个 top domain 内的,因此高层的 domain 意义不大),尽量在 domain 内选择 CPU,毕竟同一个 domain 内共享更多的 cache,然后再找到一个 idlest 的 CPU 来执行唤醒进程的.   

对于创建的和执行 exec 的进程,那就完全不需要考虑 cache 的问题了,毕竟这种进程都是全新的,不会在任何 CPU 上留下 cache 的足迹,因此,对于执行 CPU 的选择,就相对简单一点,不过也有需要注意的一点是:新执行的进程尽量和唤醒进程离得近一些,也就是能在执行唤醒进程低层的 domain 中找到 idlest cpu 就优先使用低层 domain,这样同样是出于 cache 的考虑,毕竟执行这些代码也会产生 cache 的替换,越低层 domain 之间 CPU 的操作就越可能产生可重用的 cache.   

## 负载均衡的执行
正如上文中提到的,系统中将在 tick 中断或者 CPU 无有效进程运行时触发负载均衡.  

需要注意的是,tick 中断的执行是 percpu 的,也就是说每个 CPU 是独立执行 tick 中断的,在执行负载均衡的时候也只会在当前 CPU 处于比较空闲的状态下执行,然后在各级 domain 中找到最 busy 的 CPU,当然,如果当前 CPU 并不空闲,也就没有必要执行负载均衡,那么,为什么这种负载均衡的操作不是在系统中找到最空闲的 CPU 和最 busy 的 CPU，然后直接操作对应的两个 CPU 呢?一方面,每个 CPU 都有机会自行进行负载均衡,另一方面,操作其它 CPU 很容易造成 rq lock 之间的竞争,同时产生一些无效的 cache,对系统来说是不利的,所以,对于负载均衡而言,都是各人自扫门前雪(存在特殊情况，见下文).   

当然,对于即将执行 idle 进程的 CPU 而言,已经确定了当前 CPU 即将处于空闲状态,需要做的就是找到最忙的 CPU 然后把任务迁移过来.  

确定了需要迁移的进程之后就会执行迁移,执行进程的迁移也需要考虑一些额外的细节,并不是所有情况下的进程都可以直接进行迁移:
* 如果这个进程正在 CPU 上执行,自然是不能迁移的(特殊情况有例外).
* 某些进程有 CPU 亲和性设置或者禁止了某些 CPU,进程迁移不能逾越这个规矩.  
* 如果进程即将被移出就绪队列,执行进程迁移也没什么意义,cfs 调度组的带宽控制会产生这个条件.  
* 如果进程的 cache 是热的,也不建议迁移,这并不是硬性限制,在进程迁移失败多次后可能会将其迁移.  

在找到可以迁移的进程之后,情况就变得比较简单了,直接对就绪队列上的进程执行 dequeue,然后 enqueue 到空闲 CPU 的队列上,也就完成了进程的迁移,当然,也有可能进程迁移失败,可能是系统中本来就没几个可迁移的进程,也可能是由于 CPU 亲和性的设置导致某个 CPU 被刻意地孤立,总之,负载均衡的工作到这里算是完成了.   


## 源码实现
在分析 cfs 调度器的时候,就有这样一个感慨:对于内核中的某些组件,其实现的核心思想并不复杂,但是其对应的实现确实充斥着大量的细节,如果不深入到源码中,就仅仅是了解一个大体的概念,这自然是不够的,只有分析过源代码之后,才能对该组件有一个更深刻的认识,除了更深刻的记忆之外,同样也可以看到内核源码设计的精妙所在,从而对内核有更深的理解.   

下面呢,就是源码的分析阶段,主要分为两个部分:
* sched_domain 和 sched_group 的构建
* 为新进程或者唤醒进程选择合适的 CPU
* 全局的负载均衡操作,tick 中断和即将执行 idle 进程这两种情况所执行的负载均衡有些区别,但最终都是执行 load_balance 这个主要的执行函数,因此放在一起讲,而且这是重点部分.  

### sched_domain 和 sched_group 的构建
在 start_kernel->rest_init->kernel_init->kernel_init_freeable 中,子函数 smp_prepare_cpus 负责通过扫描设备树或者从硬件中读取寄存器来构建 CPU 的拓扑结构,紧接着 smp_init 函数将会启动其它非 boot CPU,设置各个 CPU 的 present/online/active 掩码标志,以确定 CPU 的状态.  

然后,再调用 sched_init_smp 函数,从名字可以看出这是和调度相关的初始化工作,在该函数中,针对 numa 进行初始化,同时初始化 isolate map,这个 map 用于设置独立的 CPU,这些 CPU 不会被添加到调度域中,默认情况下进程不会被放到这些 CPU 上执行,除非进行人为的设置,最主要的工作在于初始化调度域和调度组,完成之后 CPU 就可以被调度器操作了.   

调度域的初始化对应的接口为:sched_init_domains(cpu_active_mask),传入的参数为 cpu_active_mask,也就是只会将 active 的 CPU 加入到调度域中,实际上,sched_domain 并非是固定的,在支持 CPU hotplug 的系统中,随着 CPU 拓扑结构的变化进行调整.  

int sched_init_domains(const struct cpumask *cpu_map)
{
	int err;
	...
	cpumask_andnot(doms_cur[0], cpu_map, cpu_isolated_map);
	err = build_sched_domains(doms_cur[0], NULL);
	register_sched_domain_sysctl();
	...
	return err;
}
在 sched_init_domains 中,cpu_map 表示 active 的 cpu 位图,需要 cpu_isolated_map 中记录的 CPU 排除,接着调用 build_sched_domains 建立整个系统的调度域,调度组也是在这个时候被构建.而 register_sched_domain_sysctl 这个函数用于将 sched_domain 相关的内核参数导出到用户空间的 /proc/sys 目录下,前提是 CONFIG_SCHED_DEBUG 和 CONFIG_SYSCTL 这两个内核选项被设置,对应的目录通常为:/proc/sys/kernel/sched_domain/cpux/domainx/.  

接下来看看最主要的一个接口 build_sched_domains,这个函数比较长,分为三个部分:
* 构建 sched_domain
* 在 sched_domain 的基础上构建 sched_group 

构建 sched_domain 的源码如下:

```c++
static int
build_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *attr){
	...
	for_each_cpu(i, cpu_map) {
		struct sched_domain_topology_level *tl;

		sd = NULL;
		for_each_sd_topology(tl) {
			sd = build_sched_domain(tl, cpu_map, attr, sd, i);
			if (tl == sched_domain_topology)
				*per_cpu_ptr(d.sd, i) = sd;
			if (tl->flags & SDTL_OVERLAP)
				sd->flags |= SD_OVERLAP;
			if (cpumask_equal(cpu_map, sched_domain_span(sd)))
				break;
		}
	}
	...
}
```

从 for_each_cpu 的循环可以看出,sched_domain 的构建是 percpu 的.  

接下来的 struct sched_domain_topology_level 结构是比较重要的,从命名中的 level 中可以看出,这个结构用来描述 domain 域的层级,而 for_each_sd_topology 用来向上轮询当前 CPU 所属的所有 domain,源码如下:

```c++
static struct sched_domain_topology_level default_topology[] = {
#ifdef CONFIG_SCHED_SMT
	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
#endif
#ifdef CONFIG_SCHED_MC
	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
#endif
	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
	{ NULL, },
};

static struct sched_domain_topology_level *sched_domain_topology =
	default_topology;

#define for_each_sd_topology(tl)			\
	for (tl = sched_domain_topology; tl->mask; tl++)
```
从上面的源码可以看出,domain 的层级被静态定义在 default_topology 中,在 arm64 的代码中 domain 只有三层,最底层为 SMT,往上是 MC 和 DIE,对于 arm64(armv8架构) 平台而言,并不支持 SMT,因此只有两层 MC 和 DIE, MC 就是 Muti Core,这个比较好理解.而在 CPU 拓扑结构的介绍中,我们并没有提及到 DIE 这个概念,而是物理 CPU 或者 socket.  

实际上,DIE 是物理上的概念,是在芯片制作过程中实体的概念,它的概念和物理 CPU/socket 差不多,但是正如我们在 CPU 拓扑结构章节中提到的,实际上每个厂商制作芯片的步骤以及概念的定义并不一样,有些概念并不是完全地一一对应,取决于实际场景,这里将 DIE/物理CPU/socket 统一为一个概念并不完全精确,但是不影响我们理解软件上的概念.比如单单对于 x86 而言,多个 core 的集合通常被称为 package,在 AMD 中又称为 Node,实际上,die 是一个相对通用的概念,才会被 linux 内核采用.  

实际上不止是 arm64 架构,即使在主线代码中,内核的 domain 层级结构并没有看到 NUMA domain 的身影,可能 NUMA domain 可以通过其它方式进行扩展,或者 NUMA 系统需要自行在该静态数组中增加对应的 domain level 项.   

有一个问题是,这些层级是根据什么来定义的呢?可以看到 default_topology 中每一项的 mask 部分,cpu_coregroup_mask 和 cpu_cpu_mask 的定义分别为:

```c++
const struct cpumask *cpu_coregroup_mask(int cpu)
{
	return &cpu_topology[cpu].core_sibling;
}
static inline const struct cpumask *cpu_cpu_mask(int cpu)
{
	return cpumask_of_node(cpu_to_node(cpu));
}
```
从源码可以看出,对于 MC domain 而言,包含当前 CPU 的 core_silbling,而对于 DIE domain,则是当前 NUMA node 中的 CPU,虽然不是 NUMA 结构系统,但是可以把整个系统当成一个单独的 NUMA node,一个 NUMA node 包含多个 DIE 这个概念是可以使用的.  

因此,domain 的构建完全是基于 CPU topology 的,这也是为什么需要花两个章节来专门介绍 CPU topology 的原因.   

构建 domain 由 build_sched_domain 完成,这个函数被包含在 for_each_cpu 和 for_each_sd_topology 中,因此,会针对每个 CPU 的每个 domain level 建立 sched_domain,然后通过 child 和 parent 指针将它们建立联系,因为 sched_domain 是 percpu 的,所以 parent 和 child 都只有一个,因此对于每个 CPU 而言, sched_domain 并不是树状结构,而是链式结构.在 build_sched_domain 中,会调用 sd_init 初始化一个全新的 sched_domain,同时设置 sched_domain 的 span 表示当前 domain 的 CPU mask.      

每构建完一层 domain 结构,都会调用 if (cpumask_equal(cpu_map, sched_domain_span(sd))) 来检查当前层级的 CPU mask是否与所有 active cpu 的 mask 一致,如果一致,就不再构建上层的 sched_domain.举个例子,在一个单 socket 包含 4 multi core 的系统中,构建的 MC domain 就包含了 4 个 CPU,其实这时候就没必要再为单个 socket 再构建一层 DIE domain 了,即使构建了,该 DIE domain 中也只包含一个子 domain,一个 group,实际上是没什么意义的,只会导致在做负载均衡的时候白白增加一次遍历成本,因此,单 socket 的 arm 系统中(不支持 SMT)通常只有一层 MC domain 域.   

build_sched_domains 的第二部分是构建 sched_group,源码如下：

```c++
static int
build_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *attr)
{
	...
	for_each_cpu(i, cpu_map) {
		for (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {
			sd->span_weight = cpumask_weight(sched_domain_span(sd));
			...
			if (build_sched_groups(sd, i))
				goto error;
			...
		}
	}
	...
}
```
针对每个层级的 domain，给 sd 的 span_weight 赋值，该值表示当前 domain 内的 CPU 数量。  

build_sched_groups 负责构建 percpu 的 sched_group，主要是设置 sg->ref 引用计数，以及 sg->cpumask，该值主要包括当前组内所有 CPU 的 mask，同时，还初始化 sg 的 capacity。  

CPU 的 capacity 代表了一个 CPU 执行程序的能力，capacity 越大，执行程序的效率越高。  

### select_task_rq —— 选择进程 CPU
select_task_rq 是核心调度部分的接口,不针对具体的调度器类,在该函数中，主要是针对进程相对 CPU 亲和性的处理，在 task_struct 结构中，有两个相关的字段：nr_cpus_allowed 和 cpus_allowed，nr_cpus_allowed 表示当前进程可以运行的 CPU 数量，而 cpus_allowed 是当前进程可运行 CPU 的掩码位，比如进程 p 可运行在 0,2,3 CPU 上，nr_cpus_allowed 为 3 而 3，cpus_allowed 为 0b00001101。  

当进程的 nr_cpus_allowed 不大于 1 时，通常表示该进程是绑定 CPU 的，因此这一类进程就不需要选择 CPU，对于其它进程，则调用对应调度器类的 select_task_rq 回调函数，该函数会返回合适执行该进程的 CPU id。  

对于 cfs 调度器，对应的 select_task_rq 回调函数为 select_task_rq_fair：

```c++
select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)
{
	...
	if (sd_flag & SD_BALANCE_WAKE) {
		record_wakee(p);
		want_affine = !wake_wide(p) && !wake_cap(p, cpu, prev_cpu)   .............................1
			      && cpumask_test_cpu(cpu, &p->cpus_allowed);
	}
	for_each_domain(cpu, tmp) {
		if (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&          ..............................2
		    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {
			affine_sd = tmp;
			break;
		}
	}
	if (affine_sd) {
		sd = NULL; 
		if (cpu == prev_cpu)
			goto pick_cpu;

		if (wake_affine(affine_sd, p, prev_cpu, sync))
			new_cpu = cpu;
	}
	if (!sd) {
 pick_cpu:
		if (sd_flag & SD_BALANCE_WAKE) 
			new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);         ........................3
	}
	...
}
```
传入第一个参数为需要操作的 task，第二个参数为 task 之前运行的进程，后面是两个 flag 参数,在 select_task_rq_fair 的前面一部分是针对唤醒进程的操作。  

注1：want_affine 这个变量从字面上的理解是是否想设置唤醒的亲和性，是不是需要让进程在本 domain 中的 CPU 上执行，这个变量成立是有条件的，包括当前 CPU 的 capacity 是否和之前 CPU 的容量(这个概念在后面讲到)差不多，当前 CPU 是否在进程的 cpus_allowed 中，同时 wake_wide(p) 返回 0，说起来这个 wake_wide 是有些复杂的，理论上来说，让进程在本地被唤醒从 cache 考虑是有益的，但是对于某些生产消费模型中，可能会出现生产进程频繁唤醒多个消费进程，出于消费进程执行效率的考虑，这种情况下最好将多个消费进程分散到多个 CPU 中快速执行完，而不是挤在一个繁忙的 CPU 上，在内核中使用 wakee_flips 来记录这种唤醒行为，通过该变量可以判断是否处于这种模型下，而特意选择较远的 CPU 执行，从而不考虑亲和性。  

注2：如果设置了 want_affine 变量，表示需要就近选择合适的 CPU，同时，如果待唤醒进程之前执行的进程也在当前 domain 下，就选中当前 domain，否则向上遍历到父级 domain。  

注3：确定了 domain，接下来的工作也就是从 domain 中找到一个合适的 CPU，这个工作由 select_idle_sibling 完成，这个函数会在共享 LLC 的 domain 内，分别调用 select_idle_core、select_idle_cpu、select_idle_smt 以找到最合适的 CPU。  


如果不是唤醒的进程，或者唤醒进程不设置亲和 CPU，那么就进入到通用的选择 CPU 的流程：

```c++
static int
select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)
{
	...
	while (sd) {                             ..............................................1
		struct sched_group *group;
		int weight;

		if (!(sd->flags & sd_flag)) {
			sd = sd->child;
			continue;
		}

		group = find_idlest_group(sd, p, cpu, sd_flag);    
		if (!group) {
			sd = sd->child;
			continue;
		}

		new_cpu = find_idlest_cpu(group, p, cpu);            .................................2
		if (new_cpu == -1 || new_cpu == cpu) {
			sd = sd->child;
			continue;
		}

		...
	}
	return new_cpu;
	...
}
```
注1：在上文判断是否亲和唤醒时，使用了 for_each_domain 循环，如果不是亲和唤醒，最终会遍历到最上层 domain，sd 的初始化则是最上层的 domain，因此，和其它 domain 遍历操作不一样的是，这里的 domain 是从下往上遍历的，在每一层 domain 寻找 idlest 的 CPU，与当前 CPU 共享更底层 domain 的 CPU 获得优先权。  

注2：寻找 idlest CPU 先后调用了 find_idlest_group 和 find_idlest_cpu 函数，从名称可以看出它们的作用，在调度器工作的过程中就一直记录着 CPU rq 的负载情况，因此通过这些负载情况可以找到 idlest 或者 busiest cpu，实际上，CPU 负载的统计并不简单，有机会的话后面会单独讲解，这里不过多赘述。  

在最后，将会返回合适的 CPU，可以发现，CPU 的选择基本遵循的规则为：考虑进程设置的 CPU 亲和性、优先本地化唤醒。  


## idle_balance
当某个 CPU 的调度器在就绪队列上找到有效的进程时，就会看看其它 CPU 上是否有多的进程，并拿过来执行，如果确实没有，才会不甘心地执行 idle 进程，这时候发起的负载均衡就是 idle_balance.  

相对于周期性的负载均衡而言，idle balance 的目的更明确，调用 load_balance 的 flag 参数为 CPU_NEWLY_IDLE，表示当前进程已经没有进程可执行了，属于最紧急的负载均衡情况。  

下面是 idle_balance 的源码实现：

```c++
static int idle_balance(struct rq *this_rq, struct rq_flags *rf)
{
	...
	unsigned long next_balance = jiffies + HZ;
	if (this_rq->avg_idle < sysctl_sched_migration_cost ||     .............................1
	    !this_rq->rd->overload) {
		sd = rcu_dereference_check_sched_domain(this_rq->sd);
		if (sd)
			update_next_balance(sd, &next_balance);
		goto out;
	}

	for_each_domain(this_cpu, sd) {
		int continue_balancing = 1;
		u64 t0, domain_cost;

		if (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {  ..........................2
			update_next_balance(sd, &next_balance);
			break;
		}

		if (sd->flags & SD_BALANCE_NEWIDLE) {
			t0 = sched_clock_cpu(this_cpu);

			pulled_task = load_balance(this_cpu, this_rq,
						   sd, CPU_NEWLY_IDLE,
						   &continue_balancing);

			domain_cost = sched_clock_cpu(this_cpu) - t0;
			if (domain_cost > sd->max_newidle_lb_cost)
				sd->max_newidle_lb_cost = domain_cost;

			curr_cost += domain_cost;
		}

		update_next_balance(sd, &next_balance);

		if (pulled_task || this_rq->nr_running > 0)                      ...........................3
			break;
	}
}

```
注1：执行 idle_balance 的根本目的在于提高程序的执行效率，迁移进程只是一种实现手段，当迁移进程本身的开销大于进程迁移产生的收益时，是不应该迁移进程的，因此，在 idle_balance 的前部分，会判断当前 CPU 的 rq 进入 idle 状态的平均时间，如果这个时间要小于进程迁移所产生的开销，就说明很大概率该 CPU 上马上就会有进程被唤醒，这时候做进程迁移就显得没那么必要了，同时，另一个条件是：如果系统中的负载本来就很轻，甚至每个 CPU 上只有或者不到一个进程，这时候下面的代码也没有太多必要执行了。在这两种情况下，直接更新一下下次执行 balance 的时间，退出即可。  

注2：确定需要执行进程迁移只有，使用 for_each_domain 从当前 domain 向上遍历到顶层 domain，调用 load_balance 试图寻找合适的可迁移的进程，但是这个过程的开销必须是可控的，为了负载均衡的操作花去太多时间并不划算，因此每执行一次 load_balance 就会记录一次时间开销，当这个开销达到一定的值，就不再继续执行负载均衡

注3：idle_balance 的目的就在于让当前 CPU 有进程可运行，并不用太贪心，因此如果有进程被迁移过来或者当前 CPU 上有进程被唤醒都会推出负载均衡的过程。  


负载均衡最核心的部分还是 load_balance 函数，这个我们将在后续统一分析。  


## trigger_load_balance
在周期性的 tick 中，同样执行负载均衡的操作，对应的函数为 trigger_load_balance，这个接口将会触发 SCHED_SOFTIRQ 软中断，该软中断对应的执行函数为 run_rebalance_domains。  

run_rebalance_domains 和 idle_balance 不同的是，传入的 flag 参数根据当前 CPU 是否处于 idle 分为 CPU_IDLE 或者 CPU_NOT_IDLE，同时，根据 sd 的对应的 balance intercal 属性确定下次执行 balance 的时间，记录 last_balance。  

主要的部分还是从当前 domain 向上遍历，调用 load_balance，是否真的需要执行进程迁移由 load_balance 函数进行判断。   



## load_balance
load_balance 是完成负载均衡工作的主要操作函数,这个函数的原型为:

```c++
static int load_balance(int this_cpu, struct rq *this_rq,
			struct sched_domain *sd, enum cpu_idle_type idle,
			int *continue_balancing);
```
该函数接受五个参数,执行负载均衡的 CPU 和 rq,这个 CPU 也是相对空闲,需要将进程移进来的 CPU.  

尽管前面分析了负载均衡的执行背景,这里有必要再重复一下:load_balance 是在一个 for_each_domain 的循环中被调用的,也就是说,在一次负载均衡中 load_balance 可能不只是调用一次,随着每一次的调用,sd 参数将会更新为 sd->parent,也就是从当前 domain 往上遍历,而 continue_balancing 这个参数其实是将是否需要继续执行 balance 的信息返回给调用者.  

同时,就目前我所理解的内核代码而言,当需要执行负载均衡时,具体的进程移动操作由空闲 CPU 完成,而不会由其它 CPU 代劳,比如 CPU1 检测到 CPU3 应该向空闲的 CPU2 迁移进程,CPU1 并不会去操作,而是等 CPU2 自己执行 idle_balance 或者周期性的 balance 来移动进程,这应该是出于 rq lock 竞争和 cache 命中的考虑.  

而 idle 这个标志位,代表当前执行 load_balance 的 CPU 的状态,主要是三个:CPU_NEWLY_IDLE 表示 CPU 刚准备进入到 idle 状态,这是在 idle_balance 时传入的标志位,CPU_IDLE 和 CPU_NOT_IDLE 则表示 CPU 的空闲与否,如果是 CPU_IDLE,也可能是 CPU 早就处于空闲状态下了,这两个标志位在周期性的负载均衡中使用,通过 idle_cpu(cpu) 可以判断 CPU 是否空闲,再传入到 load_balance 函数中.不同的标志位代表不同的负载均衡紧迫性,因此处理上也不大一样.    

load_balance 是一个很长的函数,按照一贯的风格,并不会逐行地分析源码,而是针对负载均衡的操作以及背后的原理进行逐步的分解,然后贴出关键代码用以佐证,毕竟内核代码是一直在变的,而不变的只有其背后的逻辑.  

load_balance 函数的工作主要分成以下几个部分:
* 再次确定是否有 balance 的需求
* 找到最忙的 CPU
* 将进程从最忙的 CPU 上 detach
* 将 detach 的进程 attach 到当前 CPU 上
* active balance

下面根据源码一一对这些部分进行分析.  

### struct lb_env
load balance 并不是一项简单的工作,可能需要跨 domain 进行处理,通常也包含多个 CPU 的相关操作,因此,内核中使用 struct lb_env 这个结构体来记录一些中间状态以及参数. 

其中最主要的几个成员变量为:

```c++
struct lb_env {
	struct sched_domain	*sd;   // 负载均衡当前所属的 domain

	struct rq		*src_rq;   // 进程迁移的源 rq,也就是相对繁忙的 rq
	int			src_cpu;       // 进程迁移的源 CPU

	int			dst_cpu;       // 进程迁移的目标 CPU,也就是相对空闲的 cpu
	struct rq		*dst_rq;   // 进程迁移的目标 rq

	struct cpumask		*dst_grpmask;  // 目标 sched_group 的 CPU mask 掩码位. 
	int			new_dst_cpu;     
	enum cpu_idle_type	idle;   // 执行进程迁移的 CPU 所处的状态,参考上文.
	long			imbalance;  // 该变量记录不平衡的程度,见下文分析

	struct cpumask		*cpus;  // CPU 掩码

	unsigned int		loop;
	unsigned int		loop_break;
	unsigned int		loop_max;   // 这三个变量用来控制扫描的时间,见下文分析.   

	// 这是一个链表头,被迁移的进程会挂到当前链表上,在 dst CPU attach 的时候也是从这个链表上取.
	struct list_head	tasks;   
};
```


### 确认 load_balance 的需求
对于 idle_balance 而言,负载均衡的需求自然是明确的,而对于周期性执行的负载均衡却不一样,需要先确定是否真的需要执行负载均衡,这个判断由 load_balance->should_we_balance 函数完成:

```c++
static int should_we_balance(struct lb_env *env)
{
	struct sched_group *sg = env->sd->groups;
	int cpu, balance_cpu = -1;
	if (!cpumask_test_cpu(env->dst_cpu, env->cpus))   ......................................1
		return 0;
	
	if (env->idle == CPU_NEWLY_IDLE)
		return 1;

	for_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {  
		if (!idle_cpu(cpu))
			continue;

		balance_cpu = cpu;
		break;
	}

	if (balance_cpu == -1)
		balance_cpu = group_balance_cpu(sg);

	return balance_cpu == env->dst_cpu;     ................................2
}
```

注1:env->dst_cpu 是相对空闲的当前 CPU,而 cpus 是一个 percpu 的全局变量,表示可用于负载均衡的 CPU 掩码,在这里重复进行检查的意义在于:确保 balance 的 CPU 环境是固定的,因为当前程序可能与系统的 CPU hotplug 程序并发执行,会造成一些问题,且在周期性的负载均衡中,当前程序处于 softirq 环境,优先级高于进程,如果这期间确实发生了 CPU hotplug 事件,返回0,表示本次 load_balance 函数执行终止.  

注2:从当前 domain 的当前 group 中,尝试找到第一个处于执行 idle 进程的 CPU,如果没有找到,就选择该组内 capacity 最大的 CPU,如果这个 CPU 不是 dst_cpu,返回0,同样表示本次 load_balance 函数执行终止.

因此,从这里可以看出,每一次负载均衡的 dst CPU 都是固定的(当前 CPU),如果检查到应该作为 dst 的 CPU 不是当前 CPU,就不需要执行 load_balance.  

同时,如果 should_we_balance 返回 0, load_balance 的 continue_balancing 参数也被设置为 0,表示本次整个的负载均衡操作都不需要再往下进行了.  

### 找到最忙的 CPU
在为即将加入就绪队列的进程选择 CPU 时,使用的是 find_idlest_* 相关的接口,当前操作是需要找到最忙的 CPU,相对应的,对应的接口为:

```c++
static int load_balance(int this_cpu, struct rq *this_rq,
			struct sched_domain *sd, enum cpu_idle_type idle,
			int *continue_balancing)
{
	struct sched_group *group;
	struct rq *busiest;
	group = find_busiest_group(&env);

	busiest = find_busiest_queue(&env, group);
	...
}
```
find_busiest_group 将会遍历当前 domain 的 groups 链表上挂着的所有 group,通过计算 group 的负载来确定哪个 group 是最繁忙的,这个负载是在进程调度的过程中统计的. 同时,在 find_busiest_group 中有一个非常重要的操作,就是调用 calculate_imbalance 计算 group 中的不平衡程度,这是一个 long 型变量,该值越大,表示不平衡的程度越大.  

同样的,find_busiest_queue 接口会遍历当前 group 下的 CPU,找到最繁忙的 CPU 以确定对应的 runqueue.  

关于负载统计的细节这里不作讨论.   


### 进程的 detach
find_busiest_queue 确定了负载迁移的源 CPU,下一步的工作就是从源 CPU 上取出进程到 dst CPU 上.

```c++
static int load_balance(int this_cpu, struct rq *this_rq,
			struct sched_domain *sd, enum cpu_idle_type idle,
			int *continue_balancing)
{
	...
	env.src_cpu = busiest->cpu;
	env.src_rq = busiest;

	ld_moved = 0;
	if (busiest->nr_running > 1) {

		env.flags |= LBF_ALL_PINNED;
		env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);

more_balance:
		rq_lock_irqsave(busiest, &rf);   ..................................1
		update_rq_clock(busiest);

		cur_ld_moved = detach_tasks(&env);  ...............................2
		rq_unlock(busiest, &rf);

		if (cur_ld_moved) {
			attach_tasks(&env);       ......................................3
			ld_moved += cur_ld_moved;
		}

		local_irq_restore(rf.flags);

		if (env.flags & LBF_NEED_BREAK) {
			env.flags &= ~LBF_NEED_BREAK;
			goto more_balance;
		}
}
```
如果要从 busiest CPU 上迁移进程,该 rq 就绪队列上的进程数量自然要大于 1,如果小于等于 1,该 group 可能同样存在不平衡的状态,这种情况需要交到 active balance 处理,这部分我们在下面讨论.  

不难想到,从源 CPU 上 detach 进程的操作需要遍历源 CPU 的 rq,这种操作其它 CPU rq 的行为自然是需要上锁的,同时还需要关中断,避免与中断的竞争,但是,在内核中关中断并不是推荐的做法,这直接影响到当前 CPU 执行程序的实时性,因此,即使是在无奈的情况下确实是需要关中断做一些事情,也要尽量地保证关中断的时间不能太长,这是内核中的基本原则.  

因此,内核使用了 loop/loop_max/loop_break 来管理遍历时间,从命名可以看出,loop_max 表示当前遍历一次的最长时间限制,loop_break 表示遍历超过一定的时间,休息一下,先打开中断避免对系统实时性造成太大影响,当待处理的中断返回时再继续处理,loop 变量则是用来记录遍历已经花费的时间.  

进程的 detach 工作主要由 detach_tasks 函数完成,这个函数将扫描源 CPU 的 rq,然后将合适的进程从源 CPU rq 上 dequeue,并将这些进程链接在 env->tasks 上,返回 detach 的进程数量,保存在 cur_ld_moved 中.  

detach_tasks 的实现如下:

```c++
static int detach_tasks(struct lb_env *env)
{
	struct list_head *tasks = &env->src_rq->cfs_tasks;

	if (env->imbalance <= 0)          ..........................................1
		return 0;
	
	while (!list_empty(tasks)) {

		...

		p = list_first_entry(tasks, struct task_struct, se.group_node);

		env->loop++;
		if (env->loop > env->loop_max)
			break;

		if (env->loop > env->loop_break) {         ............................2
			env->loop_break += sched_nr_migrate_break;
			env->flags |= LBF_NEED_BREAK;
			break;
		}                                                      

		if (!can_migrate_task(p, env))             ...........................3
			goto next;

		load = task_h_load(p);

		if (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)
			goto next;

		if ((load / 2) > env->imbalance)
			goto next;

		detach_task(p, env);     
		list_add(&p->se.group_node, &env->tasks);   ..........................4

		detached++;
		env->imbalance -= load;

#ifdef CONFIG_PREEMPT
		if (env->idle == CPU_NEWLY_IDLE)
			break;
#endif

		if (env->imbalance <= 0)                   ..........................5
			break;

		continue;
next:
		list_move_tail(&p->se.group_node, tasks);  
	}

	return detached;
}
```

注1:env->imbalance 表示 group 中的不平衡程度,同时,每个进程都存在一个"对 CPU 的负载贡献"的概念,CPU 的负载大小表明了 CPU 的繁忙程度,是由 CPU 就绪队列上的进程决定的,当一个进程执行时,自然是对 CPU 的负载产生了贡献,同时即使进程只是放置在 CPU 的就绪队列上,也是一样.比如一个 CPU 的 rq 上存在 3 个就绪进程,而另一个 CPU 的 rq 上存在 8 个就绪进程,谁的负载更大一目了然.   

因此,一旦一个进程被添加到就绪队列,它就对 CPU 产生负载贡献并记录下来,如果一个进程负载贡献大,那么很明显这个进程对 CPU 的需求就大,以进程的负载贡献来判定各 CPU 的负载比单纯地使用进程个数来判定 CPU 上的负载自然是要更科学.  

每 detach 一个进程,env->imbalance 将会减去该进程的 load(负载贡献),直到 env->imbalance 接近 0 的时候,表明此时两个 CPU 上的负载已经接近均衡,因此在 detach_tasks 中可以看到不少的 env->imbalance 的身影.  

注2:CPU 上有效进程被链接在 rq->cfs_tasks 上,对进程的遍历也就是对这个链表的遍历,遍历在一个 while 循环中进行,而且这个遍历是循环进行的,每遍历完一个节点都会将该节点放到链表末尾,也就是如果不使用 break,这个链表会一直循环遍历下去.每遍历一次,loop 都会自增,这个变量用于记录遍历次数,正如上文所说,当 loop 超过了 loop_max,表明关中断的时间已经足够长,遍历到此为止,loop break 和 loop_max 的区别在于,当 loop 达到 loop_break 的阈值,设置 LBF_NEED_BREAK 的标志位,表示需要休息一下,先开中断.  

注3:在负载均衡中，并不是所有的进程都可以进行迁移，内核通过 can_migrate_task 函数来判断进程是否能迁移，见下文.

注4：如果判断目标进程可以迁移，就取出该进程的 load，这个 load 是进程对于 CPU 的负载贡献，在过去的时间中，该进程表现得对进程的需求越大，load 也就越高。当设置了 LB_MIN 调度特性时，该特性会针对那些 load 值较小的进程迁移，这些进程对 CPU 的需求并不旺盛，因此迁移的必要性并不会特别大，这只是个软限制，如果没有其它进程可迁移，那么这个限制也会被打破(nr_balance_failed 表示进程迁移失败的次数，见下文 can_migrate_task 的分析)。  

同时，如果 CPU 之间的不平衡并不是很大，当前进程的 load 值除以 2 之后仍旧大于不平衡系数，也不需要迁移。  

如果进程满足了迁移的条件，就会调用 detach_task 将进程从就绪队列中移除，这个函数可以参考该文章(TODO)，然后将移除就绪队列中的进程记录在 env->tasks 链表上。  

注5：当 CPU 之间的不平衡已经被 fix，也就是 imbalance 值小于等于 0，也就不再不需要执行本次的进程迁移，心满意足地退出，然后执行下一个 domain 内的扫描或者退出。  

从最后的 list_move_tail(&p->se.group_node, tasks); 可以看出，这个遍历 rq 的操作是重复循环的。  

分析完 detach_tasks，了解了进程迁移的是如何 detach 进程之后，把视角再拉回到 can_migrate_task 函数，看看调度器如何判断进程是否合适迁移。can_migrate_task 对应的源码为：

```c++
load_balance->detach_tasks->can_migrate_task:

static int can_migrate_task(struct task_struct *p, struct lb_env *env)
{
	if (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))   ..........6
		return 0;
	
	if (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {           ...........7
		int cpu;

		schedstat_inc(p->se.statistics.nr_failed_migrations_affine);

		env->flags |= LBF_SOME_PINNED;

		if (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))
			return 0;

		for_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {
			if (cpumask_test_cpu(cpu, &p->cpus_allowed)) {
				env->flags |= LBF_DST_PINNED;
				env->new_dst_cpu = cpu;
				break;
			}
		}

		return 0;
	}

	if (task_running(env->src_rq, p)) {                                   ..........8
		schedstat_inc(p->se.statistics.nr_failed_migrations_running);
		return 0;
	}

	tsk_cache_hot = migrate_degrades_locality(p, env);
	if (tsk_cache_hot == -1)
		tsk_cache_hot = task_hot(p, env);

	if (tsk_cache_hot <= 0 ||
	    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {          ..........9
		if (tsk_cache_hot == 1) {
			schedstat_inc(env->sd->lb_hot_gained[env->idle]);
			schedstat_inc(p->se.statistics.nr_forced_migrations);
		}
		return 1;
	}

}
```
注6：如果进程即将因为 throttled 而被移出就绪队列，这种情况下就不需要再将该进程进行迁移了。  

注7：判断当前进程的 cpus_allowed 字段是否设置了 dst_cpu，如果进程不允许被运行在 dst_cpu 上，自然是不能进行迁移的。  

但是事情并没有完，调度器实际上会根据 domain 从下至上遍历 domain 进行进程的迁移工作，如果因为src CPU 上的进程因为 cpus_allowed 的设置而不对进程做任何操作，同时如果当前 CPU 是整个系统中最繁忙的 CPU 的话，那么会出现每次遍历都会选择该 CPU 作为 src CPU，因为它就是最忙的，而且也没有迁移进程出去，这个 CPU 就在遍历时被重复选中，但是却无法真正地完成迁移的工作。  

因此，为了避免出现这种情况，调度器会将这些无法迁移的进程先迁移同组的其他可迁移的 CPU 上，在这里的操作就是设置 env->new_dst_cpu 为同组的 CPU，这种操作其实是不符合迁移进程由本 CPU 完成的基本设定的，让当前运行的 CPU 处理另外两个 CPU 之间的进程迁移。 


注8：如果进程正在运行，自然是不方便做迁移的，判断的方式就是通过 p->on_cpu 标志位。  

注9：涉及到 cache 的问题，如果 cache 是热的，其实是不建议迁移的，但是当我们把视角放高一些再看，detach_tasks 中实际上是循环遍历整个进程链表，而且是重复地来回遍历，当遇到上述的几种情况而导致进程迁移失败时，env->sd->nr_balance_failed 就会自增，从 if ( env->sd->nr_balance_failed > env->sd->cache_nice_tries) 可以看出，当实在找不到合适的进程来迁移时，即使是 cache hot 的进程也只能妥协，将该进程迁移出去，毕竟进程迁移的目的是提高程序执行效率，而 cache hot 的考虑也是出于执行效率，两者冲突的时候自然是取效率更好的做法。从这里可以看出 cache hot 并不会禁止进程进程迁移，这只是一个软限制。  


## 进程的 attach
detach_tasks 函数将 detach 的进程挂在 env->tasks 链表上，并返回 detach 的进程数量，如果 detach 的数量不为 0，就需要将挂在 env->tasks 上的进程 attach 到 dst CPU 上，通常也是当前 CPU。 

attach 的操作由 attach_tasks 函数完成，这个函数并没有什么特殊的，就是将 env->tasks 上的进程一个个地取下来，然后调用 attach_task 函数，这个函数会将进程添加到就绪队列中，同时也会检查是否需要抢占 dst CPU 的 curr 进程。   

在进程 attach 完成之后，还有两部分需要处理：
* 判断 env.flags & LBF_NEED_BREAK 是否为真：detach_tasks 的执行处于关中断的环境，如果整个 task 的扫描因为达到 loop_break 的阈值而退出，会设置 LBF_NEED_BREAK 标志位，在本次 attach 完成之后，会再次调用 detach_tasks 继续进行扫描迁移工作。  
* 在 can_migrate_task 中如果因为 cpus_allowed 的原因导致无法迁移，但是出于不能让一个 CPU 被重复选中的考虑，会将进程迁移到组内其它 CPU 上，具体的迁移操作就在这里设置，具体的源码如下：

```c++
if ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {

	cpumask_clear_cpu(env.dst_cpu, env.cpus);

	env.dst_rq	 = cpu_rq(env.new_dst_cpu);
	env.dst_cpu	 = env.new_dst_cpu;
	env.flags	&= ~LBF_DST_PINNED;
	env.loop	 = 0;
	env.loop_break	 = sched_nr_migrate_break;

	goto more_balance;
}
	
```

迁移的前提是 env.imbalance 依旧大于 0，这时候会将 env.dst_cpu 设置为组内选定的其他 CPU，然后跳转到 more_balance 重新执行 detach_tasks 和 attach 函数，将 src_cpu 上的进程迁移到 dst_cpu 上。   

在内核中，这种操作不会很频繁，同时这种操作可能造成新的不平衡，需要在下次修正，不过总体来说，这种操作造成的问题要小于所带来的收益。  


## active balance
当调度器选出来的 busiest rq 上的进程不大于 1 ，也就是只有一个正在运行的进程(或者 detach 过程实在没有找到合适进程)，按理说这种情况并不是一种不平衡，因为系统的负载已经非常轻了，没有额外的进程可以进行迁移。  

上面的假设是基于 SMP 同构系统下，也就是当前 group 中的 CPU 是同质的，实际上，在一些特殊情况下，因为某些不得已的原因，进程运行在 misfit 的 CPU 上，这种情况同样会被视为一种不平衡的情况，需要将正在运行的进程移动到合适的 CPU 上。  

比如 arm 的大小核架构中，可能出现进程正运行在小核上，而大核处于 idle 状态，大核的程序运行效率是大于小核的，因此，针对这种情况，需要将小核中的进程停下来，然后将其迁移到大核中，以提高进程运行的效率。  

这就是 active balance 的主要设计思想，将正在运行的进程迁移到更合适的 CPU 上。  

CPU 在初始化时存在一个 capacity 的概念，从字面来说，capacity 表示一个 CPU 的能力，这个 capacity 的基准值由硬件提供，通过设备树传递，在内核初始化的时候设置。  

再来看看源码，在 load_balance 函数中，先会使用 need_active_balance 来判断是否真的需要执行 active balance：

```c++
static int need_active_balance(struct lb_env *env)
{
	struct sched_domain *sd = env->sd;

	if (env->idle == CPU_NEWLY_IDLE) {
		if ((sd->flags & SD_ASYM_PACKING) &&
		    sched_asym_prefer(env->dst_cpu, env->src_cpu))
			return 1;
	}

	if ((env->idle != CPU_NOT_IDLE) &&
	    (env->src_rq->cfs.h_nr_running == 1)) {
		if ((check_cpu_capacity(env->src_rq, sd)) &&
		    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))
			return 1;
	}

	return unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);
}
```
从源码中不难看出，判断是否需要执行 active balance 分为几种情况：
* 如果 dst cpu 为 CPU_NEWLY_IDLE，如果 sd->flags 设置了 SD_ASYM_PACKING，表示该 domain 内的 CPU 是异构的，这时候就需要通过判断 dst_cpu 是否更适合运行进程来确定是否需要执行 active balance。  
* 第二种情况就是直接通过 capacity 来判断哪个 CPU 更适合运行进程，毕竟 capacity 更大的 CPU 执行进程更快，没理由让大核闲着小核来做事。  
* 进程迁移失败的次数很多，这是实在找不到合适的迁移进程了，同时也不能让当前 CPU 执行 idle 进程。通常 unlikely() 函数可以看出这种情况很少出现。    

执行 active balance 具体的操作函数为 stop_one_cpu_nowait，这个函数用于停止 CPU 上的进程，实际上是唤醒 CPU 上的内核线程，在该内核线程中执行传入的回调函数，在该回调函数中将原本正在执行的进程迁移到 dst CPU 上。 


## 小节
负载均衡在两种情况下执行：
* CPU 即将执行 idle 进程，这种情况下会从下到上遍历 domain 来找到合适的进程执行迁移，但是只需要迁移一个进程就心满意足地退出。 
* 周期性的负载均衡，由 tick 中断触发，这种情况下会先判断是否需要执行进程迁移，如果需要负载均衡同样是从下到上遍历 domain，找到合适的进程迁移到当前 CPU 上。   

理论上，每个 CPU 只会处理当当前 CPU 处于相对空闲的情况，也就是把其它 busy CPU 上的进程移动到当前进程，存在特殊情况，见上文。  

同时，理论上正在运行的进程是不会被迁移的，但是当进程运行在不合适的 CPU 上，且更合适的 CPU 即将执行 idle 进程，这时候就会将正在执行的进程进行迁移。 


参考：https://developer.aliyun.com/article/767294
https://www.kernel.org/doc/html/latest/x86/topology.html

https://www.ibm.com/developerworks/cn/linux/l-cn-schldom/
https://www.kernel.org/doc/html/latest/scheduler/sched-capacity.html
