# linux cfs 调度器
在接触 linux 之前，一直接触的是 ucos、rtos 这一类实时操作系统，在切入到 linux 之后，感到困惑的第一个问题就是：在 linux 中创建的进程不需要自己管理调度的事情，比如不需要为进程分配优先级、时间片等资源，好像一切都是自动的，只需要调用 fork(),你就获得一个进程，它会在系统中持续地运行，至于其它的，不需要管，对于新手来说，这算是一种懒人的福利，只需要专注于 linux 下的应用编程即可。   

尽管最初知道 linux 的调度器必然是存在的，只是隐藏在内核深处，但是鉴于自己对 linux 的浅薄认知，刚开始也没有勇气去深究这一块，直到慢慢地接触驱动、接触内核，才慢慢地有能力阅读内核代码，去揭开内核调度器的面纱。  



## linux 调度器实现
无论是哪种操作系统，一个调度器需要解决的问题都是：如何将 CPU 公平地分配给每个需要运行的进程。  

公平自然不是指绝对的公平，而是相对的，不同的进程需要处理不同的事情，自然也会存在轻重缓急，如何让紧迫的进程快速地获取 CPU 的执行权，以及如何让更高优先级的进程获得预期的执行时间，又或者说如何让相同优先级的进程平均之间平均分配 CPU 时间。  

针对系统调度而言,进程分为实时进程和非实时进程,这两种进程在调度上属于两个世界,理论上实时进程拥有绝对的优先权,调度器在选取下一个获取执行权的进程时,总是会优先选择实时进程运行,而不是像非实时进程之间的调度那样根据优先级分配执行时间,因此,系统中的实时进程总是用来处理最重要的事情,同时它也不会长时间占用 CPU,否则其它非实时进程可能会被饿死.本系列博客不讨论或者只是带过实时进程相关的内容，专注于非实时进程的调度分析。   

单纯地从数学上计算出进程理论的运行时间并不难，如果一个 CPU 上存在两个进程，它们是相同的优先级，那么每个进程各占 50% 的时间，如果三个进程，一个进程的优先级比另外两个相同进程高一倍，那么高优先级进程占 50%，两个低优先级进程占 25%。  

实际的情况并不是抽象化的数学模型，其实现的过程中存在各种各样的阻碍，包括：
* 每种进程的运行特性并不一样,前台交互进程要求快速响应而不需要占用太多CPU时间,后台进程可以慢慢处理但是需要更多的CPU资源,对于这些进程,如何进程区分处理? 给交互进程更高的优先级，这会导致它获得更多的时间片，但是实际它大部分时间都在睡眠，但是优先级过低又会影响它的实时响应时间，后台进程也是如此。   
* 每个进程不但要求会被运行，还要求看起来它一直在运行，这也就要求所有就绪进程的运行时间在一个周期内轮转，而且这个周期还要短到让人无法感知到延迟，通常是 10ms 级别。  
* 进程切换是需要时间的，而且这个时间并不短，进程切换得越频繁这个切换开销的占比就越大，在计算进程应该获取的CPU时间时，如何尽量缩减进程的切换开销(或者说系统的管理行为所带来的开销)，显而易见的是，一个实现上越简单的调度器所带来的切换开销越小，不仅是调度器本身执行时间要短，还要考虑占用尽可能小的缓存。  
* 一个更麻烦的问题是，在实现上我们如何做到：当一个进程用完属于它的运行时间时立马停止它？理论上可以做到，稍微复杂一点的系统中都会存在多个高精度的定时器，比如为运行的进程设置一个专属的逻辑定时器，定时器到期即切换进程，这是非常理想化的，但是无疑这会为内核带来极大的复杂性，不方便扩展和维护，假设在一个繁忙的系统中存在很多个就绪进程竞争 CPU，可能造成的结果就是进程的频繁切换，系统定时器加上切换的开销占用了相当一部分时间，这必然是得不偿失。因此，linux 中使用一个固定频率的定时器来检查进程是否过期，通常这个定时器触发频率需要权衡，频率太高将带来系统开销以及功耗的上升，频率太低又会带来过大的进程时延。当然，不管如何，采用这种策略，进程的时延是必然存在的，如果进程不是以 tick 定时器的一次中断作为时间片的度量单位，这种时延会带来不准确的因素。反之如果进程以 tick 定时器的一次中断作为时间片的度量单位，那么中断以及中断下半部就会占用掉进程本该运行的时长。     
* 不是所有进程都在就绪态等着，如果一个进程处于休眠或者停止态，可能一个进程上一秒处于就绪态，刚给它分配完时间，下一刻(非常短的时间)它就进入了睡眠态，或者将一个运行周期分配给了 5 个待运行的进程，下一刻又有 5 个进程就绪了，那么这几个进程是不是需要等到下一个调度周期才能获得时间片，如果是在唤醒之初获取时间片，这个运行周期就被意外拉长了,而周期拉长意味着进程运行的间隔时间变长,进程会变得很"卡"，进程从睡眠态到就绪态的切换是非常频繁的。  
* 功耗问题的考虑，在嵌入式设备或者手持设备中，功耗是一个棘手的问题，当系统中没有进程运行时，如何处理休眠以及唤醒的问题。
* 在多核系统下，因为进程的属性不同，运行态与睡眠态之间的切换非常频繁，很可能出现某个 CPU 上门庭若市而另一个 CPU 上无人问津的情况，出于效率考虑，自然要执行进程的迁移，在这种外来者的情况下，又该如何处理该进程的运行呢？  
....

一个调度器面对的问题实在是太过于庞杂，而有些需求甚至还是相互矛盾的，如果要你来实现这个调度器,要如何平衡上面的这些问题?  

实际上,linux 中调度器的发展也是几经辗转,最初版本的调度器是最简单的做法,采用时间片的调度方式,每个进程在一个周期内根据优先级获得一个时间片，进程运行完时间片进程只能等待下一个运行周期，同时使用双向链表来管理所有进程,这就意味着,每一次的进程切换都需要遍历整个链表来选择下一个最合适运行的进程,时间复杂度为 On,如果一个大型服务器中存在1000 个进程,这种遍历的代价非常高昂,这种情况一直持续到 2.4 版本的内核,尽管在这期间调度器进行了一系列的优化,但是始终没有解决最急迫的 On 问题.同时,在 2.4 的内核中,不支持内核抢占,对多核系统的支持也不尽如人意.  

在 2.4 内核版本以及之前，linux 大多被用于服务器领域，服务器对于调度的需求并没有那么复杂，只需要最大限度地保证其吞吐量即可，对交互的要求并不高，随着 linux 逐渐进入嵌入式以及移动领域，事情就变得完全不一样了，交互体验和功耗逐渐成为非常重要的考虑因素，同时，效率和吞吐量也不能落下。  

2.6 版本算得上是 linux 的一次华丽蜕变,除了其它方面框架的巨大更新,该版本对调度器直接进行了大刀阔斧的修改,一方面增加了内核抢占的特性,这使得 linux 内核的实时性得到显著的提高,另一方面,就是将 On 调度器升级为 O1 调度器,也就是选取下一个执行进程的时间复杂度为 1,O1 调度器大体上是通过位图的方式来实现,当然还涉及到一些其它的技巧,总之,这是一步大的跨越,调度效率得到质的提升.  

上文中,我们列出了多条调度器面临的实际问题,实际上也可以看成是分时调度器面临的问题,可以看出,使用计算时间片的方式来进行调度需要处理各种各样的的复杂情况,尽管 O1 调度器将效率进行了提升,但是实际调度器的实现却是非常复杂,
复杂在于时间片的分配总是根据应用场景、或者依靠一些经验性的公式来进行计算和调整，需要针对不同的应用场景或者不同的调度情况来针对性地添加补丁(比如进程睡眠进程的奖励、交互进程的额外照顾),整个调度器变得越来越无法理解.相对静态的分时计算方式来应对动态的进程状态切换变得越来越力不从心.  

在 2.6 版本发布之后不久，基于"公平思想"的 RSDL 调度算法被初次设计并发布，这是由 Con Kolivas 所提出的，在这类调度器中，进程不再拥有一个固定的优先级和时间片，而是在一个周期内随着运行时间的推移，进程的优先级会逐渐降低，这也就保证了高优先级进程会获得优先执行权，在执行一定时间后，又会让位给原本比它优先级低的进程，而睡眠唤醒的进程因为没有运行，优先级一直保持，所以能够更快的被执行，这也是调度器设计中所期待的，同时，这种你追我赶的进程执行模式，调度器不再需要绞尽脑汁地计算进程应该占用的时间片，只需要让进程在执行的过程中保持动态的平衡即可，这种调度器确定了基于公平思想调度算法的可行性。   

RSDL 虽然提出了非常先进的思想，但是它仅仅是昙花一现，随之不久就被另一个调度器代替了，这个调度器就是 cfs，完全公平调度器，cfs 调度器的算法可以说得上是非常的优雅，实现简单、调度的效果也正如它的名称一样，同时，还有很好的扩展性，CFS 甚至一度被许多开发人员移植回 2.4 的内核。CFS 就是我们本系列文章进行分析的调度器。   

## CFS 调度器
在 CFS 调度器中，主要是以下几个变化：
1、完全移除了时间片的概念，取而代之的是虚拟时间的引入。
2、使用红黑树作为就绪队列的数据结构，红黑树的时间复杂度为 logOn，相对于 O1 调度器要稍微慢一点，但是影响并不大。 
3、弱化了 tick 定时器作为 timeline 的作用，在分时调度算法中，进程的时间片根据 tick 定时器中断进行递减，同时检查时间片是否用完需要重新调度，但是在 cfs 调度中，更新进程时间的代码在多处调用，同时不再以 tick 时长作为时间参考，时间的控制更加地精细，同时还能区别统计中断以及中断下半部与进程所占用的时间。 








[谈谈调度 - Linux O(1)](https://zhuanlan.zhihu.com/p/33461281)
