# linux 调度子系统0 - cfs 调度器简介
在接触 linux 之前，一直接触的是 ucos、rtos 这一类实时操作系统，在切入到 linux 之后，感到困惑的第一个问题就是：在 linux 中创建的进程不需要自己管理调度的事情，比如不需要为进程分配优先级、时间片等资源，好像一切都是自动的，只需要调用 fork()，你就获得一个进程，它会在系统中持续地运行，至于其它的，不需要管，对于新手来说，这算是一种懒人的福利，只需要专注于 linux 下的应用编程即可。   

尽管最初知道 linux 的调度器必然是存在的，只是隐藏在内核深处，但是鉴于自己对 linux 的浅薄认知，刚开始也没有勇气去深究这一块，直到慢慢地接触驱动、接触内核，才慢慢地有能力阅读内核代码，去揭开内核调度器的面纱。  



## linux 调度器实现
无论是哪种操作系统，一个调度器需要解决的问题都是：如何将 CPU 公平地分配给每个需要运行的进程。  

公平自然不是指绝对的公平，而是相对的，不同的进程需要处理不同的事情，自然也会存在轻重缓急，如何让紧迫的进程快速地获取 CPU 的执行权，以及如何让更高优先级的进程获得预期的执行时间，又或者说如何让相同优先级的进程平均之间平均分配 CPU 时间。  

针对系统调度而言，进程分为实时进程和非实时进程，这两种进程在调度上属于两个世界，理论上实时进程拥有绝对的优先权，调度器在选取下一个获取执行权的进程时，总是会优先选择实时进程运行，而不是像非实时进程之间的调度那样根据优先级分配执行时间，因此，系统中的实时进程总是用来处理最重要的事情，同时它也不会长时间占用 CPU，否则其它非实时进程可能会被饿死.

单纯地从数学上计算出进程理论的运行时间并不难，如果一个 CPU 上存在两个进程，它们是相同的优先级，那么每个进程各占 50% 的时间，如果三个进程，一个进程的优先级比另外两个相同进程高一倍，那么高优先级进程占 50%，两个低优先级进程占 25%。  

实际的情况并不是抽象化的数学模型，调度器的实现面临各种各样的问题，包括：
* 每种进程的运行特性并不一样，前台交互进程要求快速响应而不需要占用太多CPU时间，后台进程可以慢慢处理但是需要更多的CPU资源，对于这些进程，如何进程区分处理? 给交互进程更高的优先级，这会导致它获得更多的时间片，但是实际它大部分时间都在睡眠，但是优先级过低又会影响它的实时响应时间，后台进程也是如此。   
* 每个进程不但要求会被运行，还要求看起来它一直在运行，这也就要求所有就绪进程的运行时间在一个周期内轮转，而且这个周期还要短到让人无法感知到延迟，通常是 10ms 级别。  
* 进程切换是需要时间的，当然不仅仅是进程切换本身，还包括调度器确定下一个待运行进程的时间，而且这个时间并不短，进程切换得越频繁这个切换开销的占比就越大，如何尽量缩减进程的切换开销(或者说系统的管理行为所带来的开销)，显而易见的是，一个实现上越简单的调度器所带来的切换开销越小，不仅是调度器本身执行时间要短，还要考虑占用尽可能小的缓存。  
* 一个更麻烦的问题是，在实现上我们如何做到：当一个进程用完属于它的运行时间时立马停止它？理论上可以做到，稍微复杂一点的系统中都会存在多个高精度的定时器，比如为运行的进程设置一个专属的逻辑定时器，定时器到期即切换进程，这是非常理想化的，但是无疑这会为内核带来极大的复杂性，不方便扩展和维护，假设在一个繁忙的系统中存在很多个就绪进程竞争 CPU，可能造成的结果就是进程的频繁切换，系统定时器加上切换的开销占用了相当一部分时间，这必然是得不偿失。因此，linux 中使用一个固定频率的定时器来检查进程是否过期，通常这个定时器触发频率需要权衡，频率太高将带来系统开销以及功耗的上升，频率太低又会带来过大的进程时延。当然，不管如何，采用这种策略，进程的时延是必然存在的，如果进程不是以 tick 定时器的一次中断作为时间片的度量单位，这种时延会带来不准确的因素。反之如果进程以 tick 定时器的一次中断作为时间片的度量单位，那么中断以及中断下半部就会占用掉进程本该运行的时长。     
* 不是所有进程都在就绪态等着，如果一个进程处于休眠或者停止态，可能一个进程上一秒处于就绪态，刚给它分配完时间，下一刻(非常短的时间)它就进入了睡眠态，或者将一个运行周期分配给了 5 个待运行的进程，下一刻又有 5 个进程就绪了，那么这几个进程是不是需要等到下一个调度周期才能获得时间片，如果是在唤醒之初获取时间片，这个运行周期就被意外拉长了，而周期拉长意味着进程运行的间隔时间变长，一个进程两次被调度的间隔就被拉长，实际系统中，进程从睡眠态到就绪态的切换是非常频繁的。  
* 功耗问题的考虑，在嵌入式设备或者手持设备中，功耗是一个棘手的问题，当系统中没有进程运行时，如何处理休眠以及唤醒的问题。
* 在多核系统下，因为进程的属性不同，运行态与睡眠态之间的切换非常频繁，很可能出现某个 CPU 上门庭若市而另一个 CPU 上无人问津的情况，出于效率考虑，自然要执行进程的迁移，在这种外来进程的情况下，又该如何决定该进程的运行策略呢？  
....

一个调度器面对的问题实在是太过于庞杂，而有些需求甚至还是相互矛盾的，如果要你来实现这个调度器，要如何平衡上面的这些问题?  

实际上，linux 中调度器的发展也是几经辗转，最初版本的调度器是最简单的做法，采用时间片的调度方式，每个进程在一个周期内根据优先级获得一个时间片，进程运行完时间片进程只能等待下一个运行周期，同时使用双向链表来管理所有进程，这就意味着，每一次的进程切换都需要遍历整个链表来选择下一个最合适运行的进程，时间复杂度为 On，如果一个大型服务器中存在1000 个进程，这种遍历的代价非常高昂，这种情况一直持续到 2.4 版本的内核，尽管在这期间调度器进行了一系列的优化，但是始终没有解决最急迫的 On 问题.同时，在 2.4 的内核中，不支持内核抢占，对多核系统的支持也不尽如人意.  

在 2.4 内核版本以及之前，linux 大多被用于服务器领域，服务器对于调度的需求并没有那么复杂，只需要最大限度地保证其吞吐量即可，对交互的要求并不高，随着 linux 逐渐进入嵌入式以及移动领域，事情就变得完全不一样了，交互体验和功耗逐渐成为非常重要的考虑因素，同时，效率和吞吐量也不能落下。  

2.6 版本算得上是 linux 的一次华丽蜕变，除了其它方面框架的巨大更新，该版本对调度器直接进行了大刀阔斧的修改，一方面增加了内核抢占的特性，这使得 linux 内核的实时性得到显著的提高，另一方面，就是将 On 调度器升级为 O1 调度器，也就是选取下一个执行进程的时间复杂度为 O1，O1 调度器大体上是通过位图的方式来实现，当然还涉及到一些其它的技巧，总之，这是一步大的跨越，调度效率得到质的提升.  

上文中，我们列出了多条调度器面临的实际问题，实际上也可以看成是分时调度器面临的问题，可以看出，使用计算时间片的方式来进行调度需要处理各种各样的的复杂情况，尽管 O1 调度器将效率进行了提升，但是实际调度器的实现却是非常复杂，
复杂在于时间片的分配总是根据应用场景、或者依靠一些经验性的公式来进行计算和调整，需要针对不同的应用场景或者不同的调度情况来针对性地添加补丁(比如进程睡眠进程的奖励、交互进程的额外照顾)，整个调度器变得越来越无法理解.相对静态的分时计算方式来应对动态的进程状态切换变得越来越力不从心.  

在 2.6 版本发布之后不久，基于"公平思想"的 RSDL 调度算法被初次设计并发布，这是由 Con Kolivas 所提出的，在这类调度器中，进程不再拥有一个固定的优先级和时间片，而是在一个周期内随着运行时间的推移，进程的优先级会逐渐降低，这也就保证了高优先级进程会获得优先执行权，在执行一定时间后，又会让位给原本比它优先级低的进程，而睡眠唤醒的进程因为没有运行，优先级一直保持，所以能够更快的被执行，这也是调度器设计中所期待的，同时，这种你追我赶的进程执行模式，调度器不再需要绞尽脑汁地计算进程应该占用的时间片，只需要让进程在执行的过程中保持动态的平衡即可，这种调度器确定了基于公平思想调度算法的可行性。   

"公平思想"的意义在于，调度器不再需要对各种进程进行猜测以及区分，只需要根据当前所有进程的运行优先级进行调度，而运行时的优先级是动态调整的，意味着在进程执行的过程中会逐渐地趋向于动态的平衡，而不是像分时调度器那样需要预先分配进程时间，这种预分配要考虑的因素太多，带来更大的复杂性.  

RSDL 虽然提出了非常先进的思想，但是它仅仅是昙花一现，随之不久就被另一个调度器代替了，这个调度器就是 cfs，完全公平调度器，cfs 调度器的算法可以说得上是非常的优雅，实现简单、调度的效果也正如它的名称一样，同时，还有很好的扩展性，CFS 甚至一度被许多开发人员移植回 2.4 的内核。CFS 就是我们本系列文章进行分析的调度器。   



## CFS 调度器
相对于之前的调度器而言，在 CFS 调度器中，主要是以下几个变化：



* 完全移除了时间片的概念，取而代之的是虚拟时间的引入。
* 使用红黑树作为就绪队列的数据结构，红黑树的时间复杂度为 logOn，相对于 O1 调度器要稍微慢一点，但是影响并不大。 
* 弱化了 tick 定时器作为 timeline 的作用，在分时调度算法中，进程的时间片根据 tick 定时器中断进行递减，同时检查时间片是否用完需要重新调度，但是在 cfs 调度中，更新进程时间的代码在多处调用，同时不再以 tick 时长作为时间参考，时间的控制更加地精细，同时还能区别统计中断以及中断下半部与进程所占用的时间。 
* 对进程的调度几乎只依赖于进程的优先级这一个属性，而不再对进程的行为进行猜测来判断进程的类型区分处理，同时也去掉一些复杂的经验公式，这使得调度器的实现非常简单，也更加易于扩展.当然，对于某些特殊情况比如睡眠进程的奖励依旧存在，但是其扩展实现要比之前的调度器方便很多.




### 进程的虚拟时间

cfs 的一大创新就是引入了进程的虚拟时间，同时使用红黑树来管理这个虚拟时间，其基本策略是:cfs 调度器在选择下一个应该执行的进程之前，总是选择虚拟时间最小的进程执行，那么，虚拟时间是如何定义以及变化的呢?  

对于每个 cfs 就绪队列，都保存了一个最小的虚拟时间 min_vruntime，这个 min_vruntime 是一个单调递增的基准点，而每个在当前 cfs 就绪队列中就绪的进程，都会设置一个虚拟时间 vruntime，当发生进程的新建，唤醒或者迁移，总之一个进程被新添加到 cfs 就绪队列时，新添加进程的 vruntime 被设置为就绪队列的 min_vruntime，对于特殊情况适当给一些奖励或者惩罚，原则上是趋近于 min_vruntime.也就是说，对于进程被加入到 cfs 就绪队列时，是不区分优先级的.   

当进程在 CPU 上执行时，它的虚拟时间会逐渐增加，进程的优先级体现在虚拟时间的增长速度上，优先级越高，虚拟时间的增长速度越慢，反之亦然.比如系统中两个进程，优先级一个高一个低，同样运行 1ms 的时间，可能低优先级的 vruntime 增加了 1500，而高优先级的 vruntime 只增加了 500，因为调度器总是选择 vruntime 小进程运行，因此，高优先级的进程总是会运行更长的时间.  

同时，进程的运行并不是像预设时间片那样高优先级进程总是会优先运行完时间片，在 cfs 调度算法中，多个进程是交叉运行的，在一个调度周期内，调度算法尽量保证进程运行的最小时间，以免过于频繁的调度影响系统性能，而不保证进程的持续运行，比如在 10ms 内，两个不同优先级进程理论上 A 会运行 6ms，B 4 ms，完全可能出现 A:3ms-B:2ms-A:3ms-B:2ms.  

对于时间片的调度算法，调度周期是一个非常关键的因素，在一个调度周期内所有进程都会得到运行，而且理论上应该是运行它预设的时间片.  

在 cfs 中，使用调度延迟来描述调度的周期.一个调度延迟表示理论上所有就绪进程都得到运行的时间，但是调度延迟这个概念被弱化了，它并不是绝对的.原因在于进程的调度不再以 tick 定时器产生的中断作为刻度进行计算，而是将时间分得更细，比较常见的做法是将调度延迟设置为 6ms，8ms 或者 12ms，而一个进程运行最短的时间通常设置为调度延迟除以 8(可配置)，这个 8 代表进程数的阈值，如果 cfs 就绪队列上进程少于 8 个，调度延迟就是预设值，如果多于 8 个，调度延迟就需要线性扩展，以免每个进程的理想运行时间太短.   

那么，为什么说调度延迟是被弱化的调度周期呢?我们需要来看一看，linux 在运行过程中，会在哪些情况下检查是否需要调度:  
* tick 定时器中断中，更新并检查当前进程 vruntime，确定是否需要重新调度 
* 唤醒进程时，检查被唤醒进程是否可以抢占当前进程，这里的依据不再是 vruntime，而是主要根据优先级，同时需要系统配置的支持
* 新创建进程以及进程迁移之后，检查是否可以抢占当前进程

注意这里说的是检查点，不是抢占点，检查点指的是检查系统中是否有进程可以抢占当前进程，成为下一个待运行进程，如果存在，就设置抢占标志。而抢占点指的是当前运行的进程是否设置了抢占标志，如果有，就执行具体的抢占调度动作。 

对于 linux 的 tick 中断，通常的 HZ 配置为 100 或者 250，也就是 10ms 或者 4ms 一次定时器中断.  

我们假设下面的情况:系统中存在 A 和 B 两个进程，优先级对应占比 2:1，不存在进程的唤醒，创建和迁移，系统中调度延迟被设置为 6ms，HZ 配置为 250，理论上在一次调度延迟内，A 会运行 4ms，B 会运行 2 ms，这是系统计算的理想运行时间.  

在一轮运行中，轮到 B 运行，假设在整个 B 运行的期间，并不存在检查调度的事件发生，因此也不会发生抢占，B 会运行完整个 4ms 的时间直到下一个 tick 中断到来，在 tick 中断中，由于 B 的 vruntime 值足够大，因此 A 接替运行，那这时候 A 会运行多长时间呢?还是按照调度延迟计算的 4ms 吗?当然不是，A 会运行 8ms，因为当 A 运行 4ms 后， vruntime 值依旧小于 B，所以 A 会继续运行 4ms，直到 AB 的  vruntime 接近，因此，在 cfs 调度器中，调度延迟并不是绝对的，而"公平"才是绝对的.   

使用 vruntime 这种更小粒度的时间刻度考量运行时间的另一个好处是可以更方便地记录区分中断和进程各自运行的时间，调度得以更加公平.  

cfs 核心调度器基于上述的理念，同时还涉及到一些处理细节，这些细节隐藏在 tick定时函数，schedule()，enqueue/dequeue_task()，pick_next_task() 这些核心的函数实现中，在后续的文章中将会详细讨论这些函数的源码实现.  








[谈谈调度 - Linux O(1)](https://zhuanlan.zhihu.com/p/33461281)
